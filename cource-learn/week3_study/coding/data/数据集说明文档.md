# 数据集说明文档

本文档详细说明Week2_2课程中使用的三个数据集的特征、用途和使用场景。

---

## 一、加州房价数据集（california_housing.csv）

### 1.1 数据集概述

**数据集名称**：California Housing Dataset（加州房价数据集）

**数据集来源**：Scikit-learn内置数据集 `fetch_california_housing`

**数据规模**：20,640个样本，8个特征，1个目标变量

**问题类型**：**回归问题**（预测连续数值）

**应用场景**：房价预测、房地产分析、经济研究

### 1.2 特征说明

| 特征名称 | 英文名称 | 数据类型 | 单位 | 说明 |
|---------|---------|---------|------|------|
| 中位数收入 | MedInc | 连续值 | 万美元/年 | 该区域家庭收入的中位数，反映居民购买力 |
| 房屋年龄 | HouseAge | 连续值 | 年 | 该区域房屋的中位数年龄，影响房屋价值 |
| 平均房间数 | AveRooms | 连续值 | 个 | 每个家庭的平均房间数，反映房屋大小 |
| 平均卧室数 | AveBedrms | 连续值 | 个 | 每个家庭的平均卧室数，反映房屋结构 |
| 人口数 | Population | 连续值 | 人 | 该区域的人口总数，反映区域规模 |
| 平均入住率 | AveOccup | 连续值 | 人/户 | 每个家庭的平均入住人数，反映居住密度 |
| 纬度 | Latitude | 连续值 | 度 | 该区域的地理纬度坐标 |
| 经度 | Longitude | 连续值 | 度 | 该区域的地理经度坐标 |

### 1.3 目标变量

**目标变量名称**：PRICE（房价）

**数据类型**：连续数值

**单位**：十万美元（例如：4.526 表示 45.26万美元）

**取值范围**：通常在 0.15 到 5.0 之间

**含义**：该区域房屋价格的中位数，是我们要预测的目标

### 1.4 数据集特点

**优点**：
- 数据量适中，适合教学和实验
- 特征含义清晰，易于理解
- 包含地理位置信息（经纬度），可以探索空间关系
- 特征之间可能存在相关性，适合特征工程练习

**注意事项**：
- 数据已经过预处理，特征已经标准化
- 房价单位为十万美元，需要注意单位换算
- 经纬度特征可以用于创建新的特征（如距离市中心的距离）

### 1.5 使用场景

1. **线性回归教学**：演示如何使用线性回归预测房价
2. **特征工程**：从经纬度创建新特征（如距离特征）
3. **模型评估**：使用MSE、RMSE、R²等指标评估模型
4. **特征重要性分析**：分析哪些因素对房价影响最大
5. **数据可视化**：绘制房价分布、特征关系等图表

### 1.6 示例代码

```python
import pandas as pd
import numpy as np

# 加载数据
df = pd.read_csv('california_housing.csv')

# 查看数据基本信息
print(f"数据形状: {df.shape}")
print(f"\n前5行数据:")
print(df.head())

# 查看统计信息
print(f"\n数据统计描述:")
print(df.describe())

# 查看目标变量分布
print(f"\n房价统计:")
print(f"最小值: {df['PRICE'].min():.2f} (十万美元)")
print(f"最大值: {df['PRICE'].max():.2f} (十万美元)")
print(f"平均值: {df['PRICE'].mean():.2f} (十万美元)")
print(f"中位数: {df['PRICE'].median():.2f} (十万美元)")
```

---

## 二、鸢尾花数据集（iris.csv）

### 2.1 数据集概述

**数据集名称**：Iris Dataset（鸢尾花数据集）

**数据集来源**：Scikit-learn内置数据集 `load_iris`

**数据规模**：150个样本，4个特征，3个类别

**问题类型**：**多分类问题**（3个类别）

**应用场景**：分类算法教学、模式识别、植物分类

### 2.2 特征说明

| 特征名称 | 英文名称 | 数据类型 | 单位 | 说明 |
|---------|---------|---------|------|------|
| 花萼长度 | sepal length (cm) | 连续值 | 厘米 | 花萼的长度，花萼是花朵外层的保护结构 |
| 花萼宽度 | sepal width (cm) | 连续值 | 厘米 | 花萼的宽度 |
| 花瓣长度 | petal length (cm) | 连续值 | 厘米 | 花瓣的长度，花瓣是花朵的彩色部分 |
| 花瓣宽度 | petal width (cm) | 连续值 | 厘米 | 花瓣的宽度 |

### 2.3 目标变量

**目标变量名称**：target（类别标签）

**数据类型**：离散类别值（0, 1, 2）

**类别名称**：
- 0: setosa（山鸢尾）
- 1: versicolor（变色鸢尾）
- 2: virginica（维吉尼亚鸢尾）

**类别分布**：每个类别50个样本，**类别完全平衡**

### 2.4 数据集特点

**优点**：
- **数据集经典**：机器学习领域的"Hello World"数据集
- **数据量小**：150个样本，适合快速实验和教学
- **类别平衡**：每个类别样本数相同，适合分类算法演示
- **特征清晰**：4个特征都是可测量的物理量
- **线性可分**：部分类别可以通过线性分类器分离

**数据特点**：
- setosa（山鸢尾）与其他两类线性可分
- versicolor和virginica之间需要非线性分类器
- 特征之间可能存在相关性

### 2.5 使用场景

1. **分类算法对比**：比较不同分类算法的性能
   - 逻辑回归（Logistic Regression）
   - K近邻（KNN）
   - 支持向量机（SVM）
   - 决策树（Decision Tree）

2. **模型评估**：演示分类问题的评估指标
   - 准确率（Accuracy）
   - 混淆矩阵（Confusion Matrix）
   - 精确率、召回率、F1-score

3. **数据可视化**：绘制特征分布、散点图等
   - 花瓣长度 vs 花瓣宽度散点图
   - 不同类别的特征分布对比

4. **特征选择**：分析哪些特征对分类最重要

### 2.6 示例代码

```python
import pandas as pd
import matplotlib.pyplot as plt

# 加载数据
df = pd.read_csv('iris.csv')

# 查看数据基本信息
print(f"数据形状: {df.shape}")
print(f"\n类别分布:")
print(df['target_name'].value_counts())

# 查看各类别的特征统计
print(f"\n各类别特征统计:")
for class_name in df['target_name'].unique():
    class_data = df[df['target_name'] == class_name]
    print(f"\n{class_name}:")
    print(class_data[['sepal length (cm)', 'petal length (cm)']].describe())
```

---

## 三、不平衡数据集（imbalanced_dataset.csv）

### 3.1 数据集概述

**数据集名称**：Imbalanced Classification Dataset（不平衡分类数据集）

**数据集来源**：使用 `make_classification` 函数生成

**数据规模**：1,000个样本，10个特征，2个类别

**问题类型**：**二分类问题**（类别不平衡）

**应用场景**：演示类别不平衡问题、模型评估陷阱、不平衡数据处理

### 3.2 特征说明

| 特征名称 | 数据类型 | 说明 |
|---------|---------|------|
| feature_1 到 feature_10 | 连续值 | 10个数值特征，其中5个是信息特征（informative），5个是冗余特征 |

**特征特点**：
- **feature_1 到 feature_5**：信息特征（informative features）
  - 这些特征对分类有实际贡献
  - 包含区分不同类别的信息
  
- **feature_6 到 feature_10**：冗余特征（redundant features）
  - 这些特征由信息特征线性组合生成
  - 不提供额外的分类信息

### 3.3 目标变量

**目标变量名称**：target（类别标签）

**数据类型**：离散类别值（0, 1）

**类别分布**：
- **类别0（多数类）**：950个样本（95%）
- **类别1（少数类）**：50个样本（5%）

**不平衡比例**：19:1（类别0是类别1的19倍）

### 3.4 数据集特点

**核心特点**：**严重的类别不平衡**

**问题表现**：
- 如果模型简单地将所有样本预测为类别0，准确率可以达到95%
- 但这样的模型对类别1完全没有识别能力
- 准确率指标会误导我们，认为模型性能很好

**实际应用场景**：
- **欺诈检测**：正常交易（多数类）vs 欺诈交易（少数类）
- **疾病诊断**：健康人群（多数类）vs 患者（少数类）
- **异常检测**：正常数据（多数类）vs 异常数据（少数类）
- **垃圾邮件检测**：正常邮件（多数类）vs 垃圾邮件（少数类）

### 3.5 使用场景

1. **模型评估陷阱演示**：
   - 展示准确率在不平衡数据集上的局限性
   - 说明为什么需要其他评估指标（精确率、召回率、F1-score）

2. **混淆矩阵分析**：
   - 通过混淆矩阵发现模型的实际表现
   - 识别假阳性（FP）和假阴性（FN）

3. **不平衡数据处理方法**：
   - 过采样（Oversampling）：增加少数类样本
   - 欠采样（Undersampling）：减少多数类样本
   - SMOTE：合成少数类样本
   - 类别权重调整：给少数类更高的权重

4. **评估指标选择**：
   - 精确率（Precision）：关注误报
   - 召回率（Recall）：关注漏报
   - F1-score：平衡精确率和召回率
   - AUC-ROC：整体性能评估

### 3.6 示例代码

```python
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# 加载数据
df = pd.read_csv('imbalanced_dataset.csv')

# 查看类别分布
print("类别分布:")
print(df['target'].value_counts())
print(f"\n类别0占比: {(df['target']==0).sum() / len(df) * 100:.1f}%")
print(f"类别1占比: {(df['target']==1).sum() / len(df) * 100:.1f}%")

# 查看特征统计
print(f"\n特征统计:")
print(df.describe())

# 分析不平衡问题
print(f"\n【不平衡问题分析】")
print(f"如果模型将所有样本预测为类别0:")
print(f"  准确率 = {(df['target']==0).sum() / len(df):.4f} = {(df['target']==0).sum() / len(df) * 100:.2f}%")
print(f"  但这样的模型对类别1完全没有识别能力！")
print(f"\n因此，在不平衡数据集上，不能只看准确率，需要关注:")
print(f"  - 精确率（Precision）")
print(f"  - 召回率（Recall）")
print(f"  - F1-score")
print(f"  - 混淆矩阵")
```

---

## 四、三个数据集对比总结

### 4.1 数据集对比表

| 数据集 | 样本数 | 特征数 | 问题类型 | 类别数 | 类别平衡 | 主要用途 |
|--------|--------|--------|---------|--------|---------|---------|
| **加州房价** | 20,640 | 8 | 回归 | - | - | 回归算法教学、房价预测 |
| **鸢尾花** | 150 | 4 | 分类 | 3 | 是 | 分类算法对比、模式识别 |
| **不平衡数据集** | 1,000 | 10 | 分类 | 2 | 否（95:5） | 不平衡问题演示、评估陷阱 |

### 4.2 学习路径建议

1. **入门阶段**：使用鸢尾花数据集
   - 数据量小，易于理解
   - 类别平衡，适合学习基础分类算法
   - 特征含义清晰

2. **进阶阶段**：使用加州房价数据集
   - 学习回归问题
   - 练习特征工程
   - 掌握回归评估指标

3. **高级阶段**：使用不平衡数据集
   - 理解类别不平衡问题
   - 学习不平衡数据处理方法
   - 掌握多种评估指标的使用

### 4.3 数据集文件位置

所有数据集都保存在：`Week2_2/preparation/data/` 目录下

- `california_housing.csv` - 加州房价数据集
- `iris.csv` - 鸢尾花数据集
- `imbalanced_dataset.csv` - 不平衡数据集

---

## 五、使用建议

### 5.1 数据加载方式

**方式1：从CSV文件加载**
```python
import pandas as pd

# 加载加州房价数据
df_housing = pd.read_csv('data/california_housing.csv')
X = df_housing.drop('PRICE', axis=1)
y = df_housing['PRICE']

# 加载鸢尾花数据
df_iris = pd.read_csv('data/iris.csv')
X = df_iris[['sepal length (cm)', 'sepal width (cm)', 
             'petal length (cm)', 'petal width (cm)']]
y = df_iris['target']

# 加载不平衡数据
df_imbalanced = pd.read_csv('data/imbalanced_dataset.csv')
X = df_imbalanced.drop('target', axis=1)
y = df_imbalanced['target']
```

**方式2：从sklearn加载（原始方式）**
```python
from sklearn.datasets import fetch_california_housing, load_iris
from sklearn.datasets import make_classification

# 加载加州房价数据
housing = fetch_california_housing()
X, y = housing.data, housing.target

# 加载鸢尾花数据
iris = load_iris()
X, y = iris.data, iris.target

# 生成不平衡数据
X, y = make_classification(n_samples=1000, n_features=10, 
                           weights=[0.95, 0.05], random_state=42)
```

### 5.2 注意事项

1. **数据预处理**：
   - 加州房价数据已经标准化，可以直接使用
   - 鸢尾花数据是原始测量值，通常不需要标准化
   - 不平衡数据是生成的，特征已经标准化

2. **数据划分**：
   - 回归问题：使用 `train_test_split`，通常80%训练，20%测试
   - 分类问题：使用 `stratify=y` 保持类别比例
   - 不平衡数据：必须使用 `stratify=y` 保持不平衡比例

3. **评估指标选择**：
   - 回归问题：MSE、RMSE、MAE、R²
   - 平衡分类：准确率、精确率、召回率、F1-score
   - 不平衡分类：重点关注精确率、召回率、F1-score，不要只看准确率

---

**文档创建时间**：2024年
**数据集版本**：Scikit-learn内置数据集
**维护说明**：数据集文件保存在 `data` 目录下，可直接使用

