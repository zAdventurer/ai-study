# 你需要实现以下三个核心任务：
# 1. 将模型中的 LSTM 层替换为 GRU 层。
# 2. 将原始的 LSTM 层修改为双向 (Bidirectional) LSTM。
# 3. 为模型添加 Dropout 正则化层。

import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence

# 1. 数据准备 (无需修改)
# -----------------------------------------------------------------------------
train_data = [
    ("this movie is great", 1),
    ("i love this film", 1),
    ("what a fantastic show", 1),
    ("the plot is boring", 0),
    ("i did not like the acting", 0),
    ("it was a waste of time", 0),
    ("the storyline was predictable", 0),
    ("a truly heartwarming story", 1),
]

word_to_idx = {"<PAD>": 0}
for sentence, _ in train_data:
    for word in sentence.split():
        if word not in word_to_idx:
            word_to_idx[word] = len(word_to_idx)
vocab_size = len(word_to_idx)
idx_to_word = {i: w for w, i in word_to_idx.items()}

sequences = [torch.tensor([word_to_idx[w] for w in s.split()]) for s, _ in train_data]
labels = torch.tensor([label for _, label in train_data], dtype=torch.float32)

padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=word_to_idx["<PAD>"])

# 2. 定义模型 (*** 在这里开始修改 ***)
# -----------------------------------------------------------------------------

class SentimentClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SentimentClassifier, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # --- 任务一 & 任务二 在这里修改 ---
        # 原始的 LSTM 层
        # self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        # 任务一: 请将上面的LSTM层注释掉，并在这里实现一个 GRU 层
        # 这里我们使用 **双向 (bidirectional=True) GRU**，
        # 对应任务二中提到的“将原始的 LSTM 修改为双向”，
        # 思路是一样的：双向结构可以同时捕捉句子前后文的信息。
        # input_size = embedding_dim: 每个时间步输入的向量维度
        # hidden_size = hidden_dim: GRU 隐藏状态的维度
        # batch_first=True: 输入张量的第一个维度是 batch 大小
        # bidirectional=True: 启用双向 GRU，会有前向和后向两个方向的隐藏状态
        self.gru = nn.GRU(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True,
            bidirectional=True
        )

        # ---------------------------------


        # --- 任务三 在这里修改 ---
        # 在这里定义一个 Dropout 层
        # Dropout 的作用：在训练时随机“丢弃”一部分神经元，
        # 可以有效缓解过拟合，让模型具有更好的泛化能力。
        # p=0.5 表示有 50% 的概率将某个神经元置为 0。
        self.dropout = nn.Dropout(p=0.5)

        # ---------------------------------


        # --- 任务二 可能需要修改这里 ---
        # 原始的全连接层
        # 由于我们使用的是 **双向** GRU，每个方向都会输出一个长度为 hidden_dim
        # 的隐藏状态，因此最终拼接后的向量长度为 hidden_dim * 2，
        # 所以这里全连接层的输入维度也需要改为 hidden_dim * 2。
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        # ---------------------------------

    def forward(self, text):
        """
        前向传播函数

        参数:
            text: 张量，形状为 [batch_size, seq_len]，
                  每个元素是一个单词在词表中的索引。

        返回:
            预测的情感概率，形状为 [batch_size, 1]，
            通过 Sigmoid 压缩到 0~1 之间，用于二分类。
        """
        embedded = self.embedding(text)
        # embedded: [batch_size, seq_len, embedding_dim]

        # --- 任务一 在这里修改 ---
        # 使用 LSTM
        # lstm_out, (hidden, cell) = self.lstm(embedded)

        # 使用 GRU (需要取消注释并做相应修改)
        # gru_out: [batch_size, seq_len, hidden_dim * num_directions]
        # hidden: [num_layers * num_directions, batch_size, hidden_dim]
        gru_out, hidden = self.gru(embedded)

        # ---------------------------------

        # 根据你使用的是单向还是双向模型，hidden的形状会有所不同。
        # 对于双向模型，你可能需要拼接前向和后向的隐藏状态。
        # 这里我们简化处理，只取最后一个时间步的隐藏状态。
        # 对于单向模型, hidden 的形状是 [1, batch_size, hidden_dim]
        # 对于双向模型, hidden 的形状是 [2, batch_size, hidden_dim]

        # 对于双向 GRU：
        #   hidden[-2, :, :] 是最后一层“前向”GRU 的隐藏状态
        #   hidden[-1, :, :] 是最后一层“后向”GRU 的隐藏状态
        # 我们将二者在特征维度(dim=1)上拼接，作为整个句子的语义表示。
        forward_hidden = hidden[-2, :, :]   # [batch_size, hidden_dim]
        backward_hidden = hidden[-1, :, :]  # [batch_size, hidden_dim]
        final_hidden_state = torch.cat((forward_hidden, backward_hidden), dim=1)
        # final_hidden_state: [batch_size, hidden_dim * 2]

        # --- 任务三 在这里修改 ---
        # 在这里应用 Dropout 层
        # 只在全连接层之前对句子级别的表示做一次 Dropout，
        # 可以在不过多破坏时序结构的前提下起到正则化作用。
        final_hidden_state = self.dropout(final_hidden_state)

        # ---------------------------------

        output = self.fc(final_hidden_state)
        return torch.sigmoid(output)

# 3. 训练模型 (大部分无需修改)
# -----------------------------------------------------------------------------
EMBEDDING_DIM = 16
HIDDEN_DIM = 32
OUTPUT_DIM = 1
LEARNING_RATE = 0.05
EPOCHS = 300

# 根据你的实现实例化模型
model = SentimentClassifier(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.BCELoss()

print("开始训练模型...")
for epoch in range(EPOCHS):
    model.train()
    optimizer.zero_grad()

    predictions = model(padded_sequences).squeeze(1)
    loss = criterion(predictions, labels)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 30 == 0:
        rounded_preds = torch.round(predictions)
        correct = (rounded_preds == labels).float()
        accuracy = correct.sum() / len(correct)
        print(f'Epoch: {epoch+1:03}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item()*100:.2f}%')

print("训练完成！")

# 4. 测试模型 (无需修改)
# -----------------------------------------------------------------------------
def predict_sentiment(model, sentence):
    model.eval()
    with torch.no_grad():
        words = sentence.split()
        indexed = [word_to_idx.get(w, 0) for w in words]
        tensor = torch.LongTensor(indexed).unsqueeze(0)
        prediction = model(tensor)
        return "正面" if prediction.item() > 0.5 else "负面"

test_sentence_1 = "this film is fantastic"
print(f"'{test_sentence_1}' 的情感是: {predict_sentiment(model, test_sentence_1)}")

test_sentence_2 = "the storyline was terrible"
print(f"'{test_sentence_2}' 的情感是: {predict_sentiment(model, test_sentence_2)}")

test_sentence_3 = "i absolutely did not enjoy this movie"
print(f"'{test_sentence_3}' 的情感是: {predict_sentiment(model, test_sentence_3)}")
