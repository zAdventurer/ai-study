"""
ç¬¬å››èŠ‚ï¼šç»¼åˆå®è·µ - æ–‡æœ¬åˆ†æç³»ç»Ÿ - ç®€æ´ç‰ˆ
ç›®æ ‡ï¼šæ•´åˆæ‰€æœ‰æŠ€æœ¯ï¼Œæ„å»ºå®Œæ•´çš„æ–‡æœ¬åˆ†æç³»ç»Ÿ
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from gensim.models import Word2Vec
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import jieba
import re

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œè´Ÿå·æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·

print("="*60)
print("ç¬¬å››èŠ‚ï¼šç»¼åˆå®è·µ - æ–‡æœ¬åˆ†æç³»ç»Ÿ")
print("="*60)

# 1. æ„å»ºæ•°æ®é›†
print("1. æ„å»ºæ•°æ®é›†")
text_data = {
    'ç§‘æŠ€': ["äººå·¥æ™ºèƒ½æŠ€æœ¯å¿«é€Ÿå‘å±•", "æœºå™¨å­¦ä¹ å›¾åƒè¯†åˆ«è¿›å±•", "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯", "äº‘è®¡ç®—å¤§æ•°æ®æŠ€æœ¯"],
    'æ•™è‚²': ["åœ¨çº¿æ•™è‚²å¹³å°ä¾¿åˆ©", "æ•™å¸ˆåŸ¹å…»åˆ›æ–°æ€ç»´", "è¯¾å ‚ç»“åˆæ•™è‚²æŠ€æœ¯", "å­¦ç”Ÿç»¼åˆç´ è´¨å‘å±•"],
    'å¥åº·': ["å¥åº·é¥®é£Ÿä¹ æƒ¯é‡è¦", "å®šæœŸé”»ç‚¼å¢å¼ºå…ç–«åŠ›", "å……è¶³ç¡çœ å¤§è„‘æ¢å¤", "å¿ƒç†å¥åº·èº«ä½“å¥åº·"],
    'ç¯å¢ƒ': ["ç¯å¢ƒä¿æŠ¤äººäººè´£ä»»", "å¯å†ç”Ÿèƒ½æºå‡å°‘æ’æ”¾", "åƒåœ¾åˆ†ç±»ç¯ä¿æªæ–½", "æ£®æ—ä¿æŠ¤ç”Ÿæ€å¹³è¡¡"]
}

# æ„å»ºè®­ç»ƒæ•°æ®
documents = []
labels = []
for category, texts in text_data.items():
    for text in texts:
        documents.append(text)
        labels.append(category)

print(f"æ•°æ®é›†ï¼š{len(documents)}ä¸ªæ–‡æ¡£ï¼Œ{len(set(labels))}ä¸ªç±»åˆ«")

# 2. æ–‡æœ¬é¢„å¤„ç†
print("\n2. æ–‡æœ¬é¢„å¤„ç†")
stop_words = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å¾ˆ', 'è¦', 'ä¹Ÿ'])

def preprocess_text(text):
    """æ–‡æœ¬é¢„å¤„ç†"""
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z]', ' ', text)
    words = jieba.cut(text)
    words = [w for w in words if w not in stop_words and len(w) > 1]
    return words

processed_documents = [preprocess_text(doc) for doc in documents]
print("é¢„å¤„ç†å®Œæˆï¼Œç¤ºä¾‹ï¼š")
print(f"åŸæ–‡ï¼š{documents[0]}")
print(f"å¤„ç†åï¼š{' '.join(processed_documents[0])}")

# 3. è®­ç»ƒWord2Vecæ¨¡å‹
print("\n3. è®­ç»ƒWord2Vecæ¨¡å‹")
model = Word2Vec(
    sentences=processed_documents,
    vector_size=50,
    window=3,
    min_count=1,
    sg=1,
    epochs=50,
    seed=42
)
print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨ï¼š{len(model.wv.key_to_index)}ä¸ªè¯")

# 4. è®¡ç®—æ–‡æ¡£å‘é‡
print("\n4. è®¡ç®—æ–‡æ¡£å‘é‡")
def document_vector(doc_words, model):
    """è®¡ç®—æ–‡æ¡£å‘é‡ï¼ˆå¹³å‡æ± åŒ–ï¼‰"""
    vectors = [model.wv[word] for word in doc_words if word in model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(model.wv.vector_size)

doc_vectors = np.array([document_vector(doc, model) for doc in processed_documents])
print(f"æ–‡æ¡£å‘é‡çŸ©é˜µï¼š{doc_vectors.shape}")

# 5. æ–‡æœ¬èšç±»
print("\n5. æ–‡æœ¬èšç±»")
kmeans = KMeans(n_clusters=4, random_state=42)
cluster_labels = kmeans.fit_predict(doc_vectors)

print("èšç±»ç»“æœåˆ†æï¼š")
for i in range(4):
    cluster_docs = [j for j, label in enumerate(cluster_labels) if label == i]
    cluster_categories = [labels[j] for j in cluster_docs]
    main_category = max(set(cluster_categories), key=cluster_categories.count)
    print(f"èšç±»{i}: {len(cluster_docs)}ä¸ªæ–‡æ¡£, ä¸»è¦ç±»åˆ«: {main_category}")

# 6. æ–‡æœ¬åˆ†ç±»
print("\n6. æ–‡æœ¬åˆ†ç±»")
label_map = {label: i for i, label in enumerate(set(labels))}
numeric_labels = [label_map[label] for label in labels]

X_train, X_test, y_train, y_test = train_test_split(
    doc_vectors, numeric_labels, test_size=0.3, random_state=42
)

classifier = LogisticRegression(random_state=42, max_iter=1000)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"åˆ†ç±»å‡†ç¡®ç‡ï¼š{accuracy:.3f}")

# 7. å¯è§†åŒ–ç»“æœ
print("\n7. å¯è§†åŒ–ç»“æœ")
pca = PCA(n_components=2)
doc_vectors_2d = pca.fit_transform(doc_vectors)

# åˆ›å»ºé¢œè‰²æ˜ å°„
colors = {'ç§‘æŠ€': 'red', 'æ•™è‚²': 'blue', 'å¥åº·': 'green', 'ç¯å¢ƒ': 'orange'}
point_colors = [colors[label] for label in labels]

plt.figure(figsize=(12, 4))

# çœŸå®ç±»åˆ«åˆ†å¸ƒ
plt.subplot(1, 2, 1)
plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1], c=point_colors, alpha=0.7)
plt.title('çœŸå®ç±»åˆ«åˆ†å¸ƒ')
for category, color in colors.items():
    plt.scatter([], [], c=color, label=category)
plt.legend()

# èšç±»ç»“æœ
plt.subplot(1, 2, 2)
plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)
plt.title('èšç±»ç»“æœ')

plt.tight_layout()
plt.show()

# 8. æ–°æ–‡æœ¬åˆ†æåŠŸèƒ½
print("\n8. æ–°æ–‡æœ¬åˆ†æ")
def analyze_text(text, model, classifier, label_map):
    """åˆ†ææ–°æ–‡æœ¬"""
    processed = preprocess_text(text)
    vector = document_vector(processed, model)
    
    # é¢„æµ‹ç±»åˆ«
    pred_num = classifier.predict([vector])[0]
    reverse_map = {v: k for k, v in label_map.items()}
    predicted_category = reverse_map[pred_num]
    
    # é¢„æµ‹æ¦‚ç‡
    proba = classifier.predict_proba([vector])[0]
    
    print(f"ğŸ“„ æ–‡æœ¬ï¼š{text}")
    print(f"ğŸ¯ é¢„æµ‹ç±»åˆ«ï¼š{predicted_category}")
    print(f"ğŸ“Š å„ç±»åˆ«æ¦‚ç‡ï¼š")
    for i, prob in enumerate(proba):
        category = reverse_map[i]
        print(f"   {category}: {prob:.3f}")
    print()

# æµ‹è¯•æ–°æ–‡æœ¬
test_texts = [
    "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œå›¾åƒå¤„ç†åº”ç”¨",
    "å­¦æ ¡é‡è§†å­¦ç”Ÿèº«å¿ƒå¥åº·å‘å±•",
    "æ°”å€™å˜åŒ–å…¨çƒç”Ÿæ€ç¯å¢ƒå½±å“"
]

print("æ–°æ–‡æœ¬åˆ†æç»“æœï¼š")
for text in test_texts:
    analyze_text(text, model, classifier, label_map)