"""
ç›®æ ‡ï¼šç†è§£ç‹¬çƒ­ç¼–ç çš„å·¥ä½œåŸç†å’Œå±€é™æ€§
"""

import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œè´Ÿå·æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·

print("="*50)
print("ç¬¬ä¸€èŠ‚ï¼šç‹¬çƒ­ç¼–ç  (One-Hot Encoding)")
print("="*50)

# 1. åˆ›å»ºè¯æ±‡è¡¨
vocabulary = ['çŒ«', 'ç‹—', 'è‹¹æœ', 'æ±½è½¦', 'å­¦ä¹ ', 'å¿«ä¹']
word_to_index = {word: i for i, word in enumerate(vocabulary)}

print(f"è¯æ±‡è¡¨ï¼š{vocabulary}")
print(f"è¯æ±‡è¡¨å¤§å°ï¼š{len(vocabulary)}")

# 2. ç‹¬çƒ­ç¼–ç å‡½æ•°
def one_hot_encode(word, vocab_size):
    """å°†å•è¯è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç å‘é‡"""
    vector = np.zeros(vocab_size)
    if word in word_to_index:
        index = word_to_index[word]
        vector[index] = 1
    return vector

# 3. æ¼”ç¤ºç‹¬çƒ­ç¼–ç 
print(f"\nç‹¬çƒ­ç¼–ç æ¼”ç¤ºï¼š")
test_words = ['çŒ«', 'ç‹—', 'è‹¹æœ']
vectors = {}

for word in test_words:
    vector = one_hot_encode(word, len(vocabulary))
    vectors[word] = vector
    print(f"'{word}': {vector}")

# 4. è®¡ç®—ç›¸ä¼¼åº¦
def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0
    return dot_product / (norm1 * norm2)

# 5. ç›¸ä¼¼åº¦é—®é¢˜æ¼”ç¤º
cat_vector = vectors['çŒ«']
dog_vector = vectors['ç‹—']
apple_vector = vectors['è‹¹æœ']

sim_cat_dog = cosine_similarity(cat_vector, dog_vector)
sim_cat_apple = cosine_similarity(cat_vector, apple_vector)

print(f"\nç›¸ä¼¼åº¦è®¡ç®—ï¼š")
print(f"'çŒ«' å’Œ 'ç‹—' çš„ç›¸ä¼¼åº¦ï¼š{sim_cat_dog}")
print(f"'çŒ«' å’Œ 'è‹¹æœ' çš„ç›¸ä¼¼åº¦ï¼š{sim_cat_apple}")
print("âŒ é—®é¢˜ï¼šè¯­ä¹‰ç›¸è¿‘çš„è¯ç›¸ä¼¼åº¦ä¹Ÿæ˜¯0ï¼")

# 6. å­˜å‚¨é—®é¢˜æ¼”ç¤º
print(f"\nå­˜å‚¨é—®é¢˜ï¼š")
large_vocab_sizes = [1000, 10000, 100000]

for vocab_size in large_vocab_sizes:
    memory_mb = vocab_size * 4 / (1024 * 1024)  # 4å­—èŠ‚æµ®ç‚¹æ•°
    sparsity = (vocab_size - 1) / vocab_size * 100
    print(f"è¯æ±‡è¡¨{vocab_size:,}: {memory_mb:.2f}MB/è¯, ç¨€ç–åº¦{sparsity:.1f}%")

# 7. å¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(12, 4))
fig.suptitle('ç‹¬çƒ­ç¼–ç å‘é‡å¯è§†åŒ–')

for i, word in enumerate(test_words):
    vector = vectors[word]
    axes[i].bar(range(len(vector)), vector)
    axes[i].set_title(f"'{word}'")
    axes[i].set_ylim(0, 1.2)
    
    # æ ‡æ³¨å¯¹åº”ä½ç½®
    for j, vocab_word in enumerate(vocabulary):
        if vector[j] == 1:
            axes[i].text(j, 1.05, vocab_word, ha='center')

plt.tight_layout()
plt.show()

# 8. æ€»ç»“
print(f"\n" + "="*50)
print("ç‹¬çƒ­ç¼–ç æ€»ç»“")
print("="*50)
print("âœ… ä¼˜ç‚¹ï¼šç®€å•ç›´è§‚ï¼Œæ¯ä¸ªè¯æœ‰å”¯ä¸€è¡¨ç¤º")
print("âŒ ç¼ºç‚¹ï¼š")
print("   1. é«˜ç»´ç¨€ç–ï¼Œæµªè´¹å­˜å‚¨ç©ºé—´")
print("   2. æ— æ³•è¡¨è¾¾è¯­ä¹‰å…³ç³»")
print("   3. ç»´åº¦éšè¯æ±‡è¡¨å¢é•¿")

print(f"\nğŸ¯ è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦åˆ†å¸ƒå¼è¯å‘é‡çš„åŸå› ï¼")
print("   ä¸‹ä¸€èŠ‚å­¦ä¹ Word2Vecè§£å†³è¿™äº›é—®é¢˜")

# 9. ç®€å•å¯¹æ¯”æ¼”ç¤º
print(f"\nå¯¹æ¯”å±•ç¤ºï¼š")
print("ç‹¬çƒ­ç¼–ç ï¼šé«˜ç»´ç¨€ç–ï¼Œæ— è¯­ä¹‰")
print("ç†æƒ³è¯å‘é‡ï¼šä½ç»´ç¨ å¯†ï¼Œæœ‰è¯­ä¹‰")

# æ¨¡æ‹Ÿç†æƒ³è¯å‘é‡ï¼ˆéšæœºç”Ÿæˆï¼Œä»…ä½œæ¼”ç¤ºï¼‰
np.random.seed(42)
ideal_cat = np.random.rand(5)
ideal_dog = np.random.rand(5) + 0.3 * ideal_cat  # æ¨¡æ‹Ÿç›¸ä¼¼æ€§
ideal_apple = np.random.rand(5)

ideal_sim_cat_dog = cosine_similarity(ideal_cat, ideal_dog)
ideal_sim_cat_apple = cosine_similarity(ideal_cat, ideal_apple)

print(f"\nå‡è®¾æˆ‘ä»¬æœ‰5ç»´çš„ç†æƒ³è¯å‘é‡ï¼š")
print(f"'çŒ«' å’Œ 'ç‹—' ç›¸ä¼¼åº¦ï¼š{ideal_sim_cat_dog:.3f}")
print(f"'çŒ«' å’Œ 'è‹¹æœ' ç›¸ä¼¼åº¦ï¼š{ideal_sim_cat_apple:.3f}")
print("âœ… è¿™æ ·å°±èƒ½ä½“ç°è¯­ä¹‰å…³ç³»äº†ï¼") 