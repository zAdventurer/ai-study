# NLP

## 引言

### 1. 核心问题：计算机如何理解“人话”？

**根本矛盾**：计算机只认识数字（0和1），而人类语言是高度抽象的符号系统。

NLP（自然语言处理）的核心任务，就是搭建一座桥梁，把**离散的符号（Text）** 映射为 **连续的数值（Vector）**。

- **传统做法（符号匹配）**：类似 Word 里的 `Ctrl+F`。搜索“开心”，计算机只会找这两个字，完全不知道“快乐”、“愉悦”和它是一个意思。
- **现代做法（语义理解）**：通过向量技术，将词语映射到数学空间。在这个空间里，“开心”和“快乐”的坐标距离非常近。

### 2. NLP 技术演进的三大里程碑

理解这个历史，你才能明白为什么 Word2Vec 是革命性的。

#### **第一阶段：规则系统  —— 1950s**

- **核心逻辑**：**硬编码 (Hard Coding)**
- **实现方式**：语言学家总结出成千上万条语法规则，程序员把它们写成复杂的 `if-else` 代码。

- - *例子*：`if "主语" + "be动词" + "not": return "否定句"`

- **致命缺陷**：语言太灵活了，规则永远写不完。一旦遇到网络新词或不规范的口语，系统立刻崩溃。

#### **第二阶段：统计学习 —— 2000s**

- **核心逻辑**：**数据驱动 (Data Driven)**。不再人工写规则，而是让机器从海量文本中统计概率。
- **代表技术**：词袋模型 (Bag-of-Words)、独热编码 (One-Hot)。
- **数据结构痛点**：

- - 此时的代码逻辑通常是构建一个**稀疏矩阵**。
  - 如果你有 10 万个词，每个词就是一个长度 10 万的向量，其中 99999 个位置是 0，只有 1 个位置是 1。
  - **后果**：这就是我们在代码 `01` 中看到的**维度灾难** —— 内存占用极大，且无法计算词与词之间的相似度。

#### **第三阶段：深度学习 (Deep Learning) —— 2010s+**

- **核心逻辑**：**分布式表示**
- **技术突破**：神经网络 (Neural Networks)。
- **核心差异**：

- - 从“高维稀疏”变成了**“低维稠密”**。
  - 不再用 10 万维来表示一个词，而是压缩到 50维 或 300维 的实数向量（如 `[0.21, -0.98, 0.55...]`）。

- ![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816629360-5478df7d-333b-4b4e-be34-63d4d43e69dd.png)

## 词向量基础与独热编码

### 1. 独热编码 (One-Hot Encoding)

**核心摘要**：这是 NLP 的“远古时期”，计算机处理语言最原始、最直观的方式。虽然现在很少直接用于深度学习，但理解它对于理解“为什么要发明 Word2Vec”至关重要。

#### 定义

**通俗理解**：就好比给班里的每个同学发一个唯一的学号。独热编码就是给字典里的每一个词，分配一个唯一的、很长的“身份证号码”。这个号码的特点是：**只有一位是 1，其他全是 0**。

**专业表述**：独热编码是将离散的符号（词语）映射为高维向量的一种方法。如果词表大小为 ![img](https://cdn.nlark.com/yuque/__latex/459f3c80a50b7be28751b0869ef5386a.svg)，则每个词都被表示为一个长度为 ![img](https://cdn.nlark.com/yuque/__latex/459f3c80a50b7be28751b0869ef5386a.svg) 的向量。对于第 ![img](https://cdn.nlark.com/yuque/__latex/2443fbcfeb7e85e1d62b6f5e4f27207e.svg) 个词，向量的第 ![img](https://cdn.nlark.com/yuque/__latex/2443fbcfeb7e85e1d62b6f5e4f27207e.svg) 个位置为 1，其余位置均为 0。

------

#### 原理与图解

假设我们的词库里只有 **4 个词**：`["苹果", "香蕉", "手机", "电脑"]`。

计算机是这样存储它们的：

| **词语 (Word)** | **索引 (Index)** | **独热向量 (One-Hot Vector)** | **说明**    |
| --------------- | ---------------- | ----------------------------- | ----------- |
| **苹果**        | 0                | `[1, 0, 0, 0]`                | 第 1 位是 1 |
| **香蕉**        | 1                | `[0, 1, 0, 0]`                | 第 2 位是 1 |
| **手机**        | 2                | `[0, 0, 1, 0]`                | 第 3 位是 1 |
| **电脑**        | 3                | `[0, 0, 0, 1]`                | 第 4 位是 1 |

**机制总结**：

它本质上是一个**“位置索引”**。计算机只知道“苹果”是 0 号坑位的词，“手机”是 2 号坑位的词，完全不关心它们的意思。

------

#### 核心缺陷

##### 维度灾难

- **现象**：如果你的词库有 10 万个词（中文常用词），那么每个词的向量长度就是 100000。
- **后果**：计算量指数级爆炸。仅仅为了表示一个词，就要开辟这么大的内存空间，效率极低。

##### 3.2 数据稀疏

- **现象**：在一个长度为 10 万的向量里，只有 1 个位置是 `1`，剩下 99,999 个位置全是 `0`。
- **后果**：这就像运送一车空气，只有一粒米是有用的。这种“稀疏矩阵”极大地浪费了存储和计算资源。

##### 3.3 语义鸿沟 —— **最致命的问题**

- **现象**：在数学上，任意两个独热向量都是**正交**的（垂直的）。

- - 计算点积：`[1, 0, 0, 0] · [0, 1, 0, 0] = 0`
  - 计算距离：无论哪个词，它们之间的欧氏距离都是一样的 (![img](https://cdn.nlark.com/yuque/__latex/ad9f8ac78aa46f4d4950680c3a3e9915.svg))。

- **后果**：计算机认为“苹果”和“香蕉”的相似度，跟“苹果”和“手机”的相似度是一模一样的（都是 0）。**计算机完全无法理解“词义相似性”。**

------

#### 4. 优点与适用场景

虽然缺点多，但它并没有被淘汰，依然有它的用武之地。

- **优点**：

- - **实现简单**：不需要训练模型，查字典就能生成。
  - **离散明确**：在某些不需要计算相似度的分类任务中（比如把文本特征转化为 ID 输入给逻辑回归），它依然很有效。

- **适用场景**：

- - **小规模数据**：词表非常小（比如只有 100 个类别标签）。
  - **不需要语义泛化**：比如只是统计某个词是否出现（词袋模型 Bag-of-Words）。

![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816703993-92a5ec80-9406-4f23-a69d-45c70d9cc87a.png)

------

#### 5. Python 代码示例

```python
import numpy as np
from sklearn.preprocessing import OneHotEncoder

# 1. 准备数据：假设我们有三句话（分词后）
# 注意：OneHotEncoder 接收的是二维数组
data = [
    ['苹果'], 
    ['香蕉'], 
    ['手机'], 
    ['电脑']
]

# 2. 初始化编码器
# sparse_output=False 意味着直接输出数组，方便我们看（实际上线通常用 True 节省内存）
encoder = OneHotEncoder(sparse_output=False)

# 3. 训练并转换
one_hot_vectors = encoder.fit_transform(data)

# 4. 打印结果
print("--- 词表对应的类别 ---")
print(encoder.categories_)

print("\n--- 生成的 One-Hot 向量 ---")
print(one_hot_vectors)

# 5. 验证“正交性”（语义鸿沟）
vec_apple = one_hot_vectors[0]  # 苹果
vec_banana = one_hot_vectors[1] # 香蕉

# 计算点积（相似度的一种简单衡量）
dot_product = np.dot(vec_apple, vec_banana)
print(f"\n苹果和香蕉的点积: {dot_product}")
# 结果一定是 0.0，说明计算机认为它们毫无关系
```

------

### 2. 分布式表示

**核心摘要**：如果不使用孤立的 ID，而是用一组“坐标”来表示词语，世界会发生什么变化？

分布式表示的核心思想是：**一个词的含义，由它周围的词决定**（Distributional Hypothesis）。

#### 1. 定义

**通俗理解**：

不再给词语发“身份证号”，而是给它一张**“性格分析表”**。

比如我们用 3 个特征来描述词语：`[是否是生物, 是否是食物, 体积大小]`。

- **苹果**：`[0.1, 0.9, 0.2]` （不是生物，是食物，体积小）
- **香蕉**：`[0.1, 0.95, 0.3]` （不是生物，是食物，体积小）
- **老虎**：`[0.9, 0.1, 0.8]` （是生物，不能吃，体积大）

你会发现，**苹果**和**香蕉**的“性格数据”长得很像，而跟**老虎**差别很大。计算机通过对比这些数据，就能瞬间明白：“哦，苹果和香蕉是一类的！”

**专业表述**：

分布式表示是将词语映射到一个**低维**（Low-dimensional）、**稠密**（Dense）、**连续**（Continuous）的实数向量空间中。每个维度不再代表具体的 ID，而是该词在潜在语义空间中的特征值。

------

#### 2. 核心原理

##### 2.1 几何空间映射

想象在一个巨大的多维空间里：

- 所有的“水果词”都聚在一起。
- 所有的“职业词”都聚在另一个角落。
- 词与词之间的距离（Distance），直接反映了它们语义的相似程度。

##### 2.2 著名的“国王与王后”算式

这是分布式表示最神奇的地方——**词向量支持数学运算**。

如果我们把“国王”的向量，减去“男人”的向量，再加上“女人”的向量，结果会惊人地接近“王后”。

![img](https://cdn.nlark.com/yuque/__latex/248e76d9f8ab5d6943919a2b8a820ebf.svg)

这说明模型不仅记住了词，还学会了**类比关系**（性别、时态、国家-首都等）。

------

#### 3. 优缺点分析

针对上一节“独热编码”的三个死穴，分布式表示逐一击破：

| **维度** | **独热编码 (旧)**  | **分布式表示 (新)**  | **评价**                                   |
| -------- | ------------------ | -------------------- | ------------------------------------------ |
| **维度** | 100,000 维 (高维)  | 50 ~ 300 维 (低维)   | **解决“维度灾难”**，极大节省内存。         |
| **数据** | 只有 0 和 1 (稀疏) | 小数，如 0.23 (稠密) | **解决“数据稀疏”**，每一位都蕴含信息。     |
| **语义** | 互相垂直 (正交)    | 距离反映相似度       | **解决“语义鸿沟”**，计算机终于“懂”词义了。 |

- **唯一的缺点：可解释性差**

- - 在独热编码中，第 1 位是 1 就代表是“苹果”，含义很明确。
  - 在词嵌入中，向量的第 1 位是 `0.234`，这个数字具体代表什么（是颜色？是词性？），人类很难直接看懂，通常被视为“黑盒”。

![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816730459-339747a0-cf2a-43d4-8b97-1e93e9b3354a.png)

------

#### 4. 使用场景

只要涉及**深度学习**的 NLP 任务，它几乎是标配：

- **机器翻译**：比如 Google 翻译，必须理解“Apple”有时是水果，有时是公司。
- **情感分析**：判断“这电影真**烂**”和“这电影真**差**”是同一个意思。
- **推荐系统**：把用户看过的文章转换成向量，计算相似度，推荐内容相似的文章。

------

#### 5.Python 代码示例

既然词变成了向量，我们怎么算两个词像不像呢？

最常用的方法是 **余弦相似度 (Cosine Similarity)**。

**数学小贴士**：

- 余弦相似度 = 1：两个词完全一样（重合）。
- 余弦相似度 = 0：两个词毫无关系（垂直）。
- 余弦相似度 = -1：两个词完全相反（反向）。

```python
import numpy as np
from numpy.linalg import norm

# 1. 模拟三个词的“分布式向量” (假设这是模型训练好的结果)
# 维度：[是否生物, 是否食物, 是否电子产品]
vec_apple  = np.array([0.1, 0.9, 0.0])  # 苹果
vec_banana = np.array([0.1, 0.8, 0.0])  # 香蕉
vec_phone  = np.array([0.0, 0.0, 0.9])  # 手机

# 2. 定义余弦相似度公式
def cosine_sim(v1, v2):
    # 公式：(A . B) / (|A| * |B|)
    return np.dot(v1, v2) / (norm(v1) * norm(v2))

# 3. 计算相似度
score_1 = cosine_sim(vec_apple, vec_banana)
score_2 = cosine_sim(vec_apple, vec_phone)

print(f"苹果 vs 香蕉 相似度: {score_1:.4f}")  
# 结果: 0.9939 (非常高，说明是一类)

print(f"苹果 vs 手机 相似度: {score_2:.4f}")  
# 结果: 0.0000 (非常低，说明没关系)

# 对比上一节独热编码算出来的 0.0，这就是质的飞跃！
```

------

### 3. 数学基石

**核心摘要**：词向量的本质是**空间几何**。理解了距离和方向，就理解了计算机眼中的“语义”。

#### 3.1 欧氏距离 (Euclidean Distance)

这是最符合我们直觉的“物理距离”。想象你在纸上画两个点，用直尺量出来的长度就是欧氏距离。

- **公式**：

假设有两个词向量 ![img](https://cdn.nlark.com/yuque/__latex/de951302f41d4707b9d80ca1af34dd0f.svg) 和 ![img](https://cdn.nlark.com/yuque/__latex/54f5fb1b07a88521e7b036e3bc7a5e33.svg)（以 2 维为例）：

![img](https://cdn.nlark.com/yuque/__latex/d7de69f7e8f2139016b5132a515490bf.svg)

- **直观意义**：

空间中两个点离得越近，数值差异越小。

- **缺点（NLP 中的大坑）**：

欧氏距离对**向量的长度（模长）** 非常敏感。

- - *例子*：有一篇文章提到了 1 次“苹果”，另一篇文章提到了 100 次“苹果”。这两个“苹果”向量的方向可能一样（语义相同），但长度差了 100 倍。如果用欧氏距离算，计算机会觉得它们差别很大。
  - *结论*：所以在 NLP 中，我们通常**不优先**使用欧氏距离。

#### 3.2 余弦相似度 (Cosine Similarity) —— **NLP 的黄金标准**

比起“距离”，我们更关心“方向”。

- **原理**：

利用向量夹角的余弦值来衡量相似度。

- - 如果两个词向量方向一致（夹角 ![img](https://cdn.nlark.com/yuque/__latex/f1c3deea7e00ba063d1c7d0a8702aae3.svg)），余弦值为 1（完全相似）。
  - 如果两个词向量方向垂直（夹角 ![img](https://cdn.nlark.com/yuque/__latex/3f335a7d9478145b7eba9371ca128373.svg)），余弦值为 0（毫无关系）。
  - 如果两个词向量方向相反（夹角 ![img](https://cdn.nlark.com/yuque/__latex/bddcb31928d83123df60d370832b885a.svg)），余弦值为 -1（完全相反）。

- **公式**：

![img](https://cdn.nlark.com/yuque/__latex/23ea070d674699d7deec354a977a2bc4.svg)

- - 分子 ![img](https://cdn.nlark.com/yuque/__latex/6bec5cdb52ac0330f3381950572b5b02.svg) 是**点积**（Dot Product）。
  - 分母 ![img](https://cdn.nlark.com/yuque/__latex/0b0b5c9e9af3b2fe353cf28353aa8c8f.svg) 是两个向量模长的乘积（用于归一化，消除了长度的影响）。

- **为什么它更好？**

它忽略了向量的长度（词频），只看方向（语义）。无论“苹果”出现了 1 次还是 100 次，它们在语义空间指向的方向是一致的，余弦相似度就能判定它们是同一个东西。

#### 3.3 向量加减法 (Vector Arithmetic)

这是分布式表示最迷人的特性：**语义的可加性**。

- **原理**：

向量的加减法遵循“平行四边形法则”。

当我们做 `King - Man` 时，我们实际上是从“国王”出发，减去了“男性”这个特征向量，剩下的可能就是“皇室 + 统治者”这种纯粹的概念。

然后 `+ Woman`，就是把“女性”的特征加回去，最终落脚点自然就到了“Queen”附近。

- **公式体现**：

![img](https://cdn.nlark.com/yuque/__latex/d0e6c52eec5f0ae6217e4e42fb69ddc9.svg)

------

## Word2Vec 深度解析🌟

**核心摘要**：2013 年，Google 的 Mikolov 团队发表了 Word2Vec，彻底引爆了 NLP 领域。

它不是一个单一的算法，而是一组架构。它的核心思想是：**“一个词的含义，可以由经常跟它在一起出现的词来定义。”**

### 1. 两种核心架构

Word2Vec 提供了两种训练模式，就像武林中的“内功”和“外功”，虽然路径不同，但目的都是为了练成“词向量”。

#### 1.1 连续词袋模型 CBOW (Continuous Bag of Words) —— “完形填空”高手

- **定义**：

根据上下文（Context）来预测中间的那个词（Target）。

- **原理**：

想象一个滑动窗口在句子上移动。

- - **输入**：周围的词（比如左边 2 个，右边 2 个）。
  - **输出**：中间被挖掉的那个词。
  - **例子**：

句子：“今天 **天气** 真 **好**，我们 **去** 踢球。”

如果我们要预测“好”，CBOW 会把“天气”、“真”、“我们”、“去”作为输入，让模型去猜中间是“好”。

- **通俗类比**：

这就好比做英语考试的**“完形填空”**。如果你看到“**__** is the capital of France”，虽然中间空了，但根据前后的词，你大概率能填出“Paris”。![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816766092-3e26518b-cbf3-4c5c-b5af-b2d2d7c01ed7.png)

#### 1.2 跳字模型 Skip-gram —— “头脑风暴”大师

- **定义**：

根据中间的那个词（Target），来预测它周围的词（Context）。

- **原理**：

这是 CBOW 的逆过程。

- - **输入**：中间的一个词。
  - **输出**：它周围可能出现的词。
  - **例子**：

给模型输入“苹果”。

模型需要预测出它周围可能会出现：“手机”、“乔布斯”、“好吃”、“水果”等。

- **通俗类比**：

这就像玩**“你画我猜”**或者**“词语联想”**。提到“周杰伦”，你会联想到什么？“奶茶”、“双截棍”、“流行音乐”。![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816781082-4c7f89f2-fe34-4d6e-b5e1-a3f60f6455b7.png)

------

### 2. 谁更强？(CBOW vs Skip-gram 对比)

| **维度**       | **CBOW (完形填空)**        | **Skip-gram (词语联想)**             |
| -------------- | -------------------------- | ------------------------------------ |
| **训练速度**   | **快** (Fast)              | **慢** (Slow)                        |
| **原理逻辑**   | 多个词预测 1 个词 (多对一) | 1 个词预测多个词 (一对多)            |
| **生僻词效果** | 较差 (容易被高频词平滑掉)  | **好** (对低频词敏感)                |
| **适用场景**   | 语料库很大、追求速度时     | 语料库较小、追求生僻词准确度时       |
| **业界主流**   | -                          | **Skip-gram** (通常效果更好，更常用) |

------

### 3. 训练黑科技：滑动窗口 (Sliding Window)

**核心摘要**：神经网络不能直接读文章，它需要明确的“输入”和“输出”。滑动窗口就是那个把**非结构化文本**转化成**成对训练数据**的“扫描仪”。

#### 3.1 什么是滑动窗口？

想象你在读一行文字，手里拿了一个矩形的框，一次只能看到几个词。你把这个框从句子的开头慢慢移到结尾，这个过程就叫**滑动窗口**。

- **核心参数**：`Window Size`（窗口大小）。

- - 如果 Size = 2，意味着不仅看当前这个词，还要看它**左边 2 个词**和**右边 2 个词**。

#### 3.2 数据生成全过程 (图解复现)

为了讲清楚，我们用一个最经典的句子为例：

**Sentence**: "The quick **brown** fox jumps over the lazy dog."

假设我们现在的模型是 **Skip-gram**（用中心词预测周围词），窗口大小设为 **2**。

**第一步：窗口停在 "brown" 上**

- **中心词 (Target)**：`brown`
- **窗口视野**：`[The, quick, brown, fox, jumps]`
- **上下文 (Context)**：

- - 左边 2 个：`The`, `quick`
  - 右边 2 个：`fox`, `jumps`

- **生成的训练样本 (Input, Label)**： 模型会得到 4 组数据，告诉它：“只要看到 brown，周围就很大概率会出现以下这些词”：

1. 1. (brown, **The**)
   2. (brown, **quick**)
   3. (brown, **fox**)
   4. (brown, **jumps**)

**第二步：窗口向右滑动一格，停在 "fox" 上**

- **中心词 (Target)**：`fox`
- **窗口视野**：`[quick, brown, fox, jumps, over]`
- **上下文 (Context)**：`quick`, `brown`, `jumps`, `over`
- **生成的训练样本**：

1. 1. (fox, **quick**)
   2. (fox, **brown**)
   3. (fox, **jumps**)
   4. (fox, **over**)

#### 3.3 为什么这一步至关重要？

通过这种滑动机制，原本杂乱无章的文章，被瞬间转换成了**成千上万对** `**(X, Y)**` **训练数据**。

- 如果有 100 万个句子的语料库，滑动一遍，就能生成几千万条训练样本。
- 神经网络看着这些样本，不断调整参数，最终学会：*“哦，'fox' 和 'jumps' 经常成对出现，它们的向量应该靠得近一点。”*

#### 3.4 窗口大小的影响

- **小窗口** ：

- - **关注点**：词法和句法关系。
  - **效果**：模型能学到“Run”后面通常接“Fast”（搭配关系），或者“High”和“Higher”性质相近。
  - **适用**：需要精确捕捉语法功能的任务（如词性标注）。

- **大窗口**：

- - **关注点**：主题和语义关系。
  - **效果**：模型能学到“Apple”和“Phone”经常在同一个长句子里出现，即使它们隔得有点远。
  - **适用**：需要理解从属关系或主题分类的任务。

------

### 4. 训练加速：负采样 (Negative Sampling)

**核心摘要**：面对 10 万个词的庞大词表，我们不再“雨露均沾”地计算每一个词的概率，而是“弱水三千，只取一瓢”，只更新一小部分词的权重。

#### 4.1 为什么要用它？

还记得 Skip-gram 的任务吗？

**输入**：“Fox” ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **预测**：“Jumps”。

如果没有负采样，模型在输出层会面临一个**巨大的计算灾难**：

1. **Softmax 的噩梦**：为了算出“Jumps”的概率，模型必须把词表中 **10 万个词**（Apple, Car, Zoo...）全部算一遍，算出它们的概率得分，确保证加起来等于 1。
2. **反向传播的灾难**：当我们发现预测错了，需要调整参数时，我们需要把这 **10 万个词** 对应的权重全部更新一遍。
3. **结论**：每预测一个词就要更新 10 万个参数，这在海量数据下是**不可接受的**。

#### 4.2 负采样的原理

**核心思想**：将“多分类问题”转化为“二分类问题”。

- **旧思路（多分类）**：

- - 问题：在 10 万个词里，哪一个是正确答案？
  - 计算量：100,000 次。

- **新思路（二分类 + 负采样）**：

- - 我们不再问“哪个词是正确的”，而是构建几组数据，问模型“这对词是不是真的？”
  - **正样本 (Positive Sample)**：`(Fox, Jumps)` ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **标签：1 (是)**
  - **负样本 (Negative Sample)**：我们随机从词表里抽几个**明显不对**的词（比如抽 5 个）。

- - - `(Fox, Airplane)` ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **标签：0 (不是)**
    - `(Fox, Tofu)` ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **标签：0 (不是)**
    - `(Fox, Winter)` ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **标签：0 (不是)**

...

- - **现在的任务**：模型只需要让 `(Fox, Jumps)` 的概率接近 1，让这 5 个负样本的概率接近 0 即可。
  - **计算量**：从 100,000 次骤降到 **6 次**（1 个正样本 + 5 个负样本）。

#### 4.3 通俗类比

想象你是老师（模型），正在批改全班 100 个学生（词表）的作业。

- **Softmax (旧方法)**：

不管是谁回答了问题，你都要把全班 100 个人的作业本全部拿出来，一个个检查、打分、写评语。

- - *累死老师（计算量大）。*

- **负采样 (新方法)**：

现在谁回答了问题（比如小明），你只把**小明**（正样本）叫过来批改。

为了防止你偷懒，校长要求你再随机抽查 **5 个没回答问题的同学**（负样本），看看他们有没有在捣乱。

- - *现在你只需要批改 6 本作业，效率提升了几十倍！*

#### 4.4 关键细节

- **怎么选负样本？**

并不是完全随机选的。**频率高**的词（如 "the", "a"）被选为负样本的概率会大一些，**频率低**的生僻词被选中的概率小一些。这就好比老师抽查时，更容易抽查那些平时爱捣乱的学生。

- **负样本选多少个？**

- - **小数据**：建议 5-20 个。
  - **大数据**：建议 2-5 个（数据够多了，稍微抽几个就行）。

#### 4.5 Python 代码示例

```python
from gensim.models import Word2Vec

# 准备语料
sentences = [["cat", "say", "meow"], ["dog", "say", "woof"]]

# 初始化模型
model = Word2Vec(
    sentences,
    vector_size=100,  # 向量维度
    window=5,         # 滑动窗口大小
    min_count=1,      # 忽略出现次数少于 1 的词
    sg=1,             # sg=1 使用 Skip-gram; sg=0 使用 CBOW
    negative=5,       # 【关键参数】开启负采样，每次抽 5 个负样本
    epochs=10         # 训练轮数
)

# 训练完成后，查看词向量
print("Cat vector:", model.wv['cat'])
```

------

### 3.4 Word2Vec 总结 

| **核心组件** | **作用**                                             | **选择建议**                                           |
| ------------ | ---------------------------------------------------- | ------------------------------------------------------ |
| **架构**     | **Skip-gram** (推荐) vs CBOW                         | Skip-gram 对生僻词更好，更常用。                       |
| **优化**     | **Negative Sampling** (推荐) vs Hierarchical Softmax | 负采样计算更快，效果通常更好。                         |
| **窗口**     | **Window Size**                                      | 想抓语法选小窗口 (2)，想抓主题选大窗口 (5-10)。        |
| **维度**     | **Vector Size**                                      | 一般设 100-300。太小表达不够，太大容易过拟合且计算慢。 |

------

## 其他词向量模型

**核心摘要**：本节介绍 Word2Vec 的两位最强竞争者。

- **GloVe**：擅长利用全局统计信息。
- **FastText**：擅长处理单词内部结构（词根词缀），是解决生僻词的神器。

### 1. GloVe (Global Vectors for Word Representation)

#### 1.1 定义

由斯坦福大学（Stanford）团队提出。它结合了“矩阵分解”（全局统计）和“局部窗口”（Word2Vec）的优点。

#### 1.2 原理：共现矩阵 (Co-occurrence Matrix)

GloVe 的核心思想是：**“统计胜于预测”**。

它不像 Word2Vec 那样拿着滑动窗口一遍遍去“猜”，而是先通读全文，统计出所有词的**共现概率**。

- **例子**：

假设我们要区分“冰（Ice）”和“蒸汽（Steam）”。

- - **共现词 "Solid"（固体）**：与 Ice 出现的概率大，与 Steam 出现的概率小。比值 > 1。
  - **共现词 "Gas"（气体）**：与 Ice 出现的概率小，与 Steam 出现的概率大。比值 < 1。
  - **共现词 "Water"（水）**：与两者出现的概率都很大。比值 ![img](https://cdn.nlark.com/yuque/__latex/b87101e34279ae8a2c60bd9080f20799.svg) 1。
  - **共现词 "Fashion"（时尚）**：与两者出现的概率都很小（无关）。比值 ![img](https://cdn.nlark.com/yuque/__latex/b87101e34279ae8a2c60bd9080f20799.svg) 1。

GloVe 直接将这些**比值关系**编码进向量里，使得向量天然包含了全局的统计规律。

#### 1.3 优缺点

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **训练快**（对于特定数据集）：利用了矩阵分解技术，通常比 Skip-gram 快。 **全局性好**：对低频词的统计更准，因为利用了整个语料库的信息。 |
| **缺点** | **内存占用大**：需要构建一个巨大的共现矩阵（词表 ![img](https://cdn.nlark.com/yuque/__latex/e0dc12bed73d85d0c6071ab9b5ed4bf3.svg) 词表），非常吃内存。 **静态**：和 Word2Vec 一样，训练完就定死了，无法解决多义词问题。 |

![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816808803-4964151a-da74-44e0-9ea4-fbe9b7a13364.png)

------

### 2. FastText (Facebook AI) —— **工业界宠儿**

#### 2.1 定义

由 Word2Vec 的原作者 Mikolov 转投 Facebook 后提出。它是 Word2Vec 的升级版，最大的改进在于：**它不再把“单词”作为最小单位，而是把“字符（Character）”作为最小单位。**

#### 2.2 原理：子词嵌入

- **Word2Vec 的做法**：

`"apple"` 就是一个独立的 ID，跟 `"apples"` 没有半毛钱关系。

- **FastText 的做法**：

它把单词拆解成 **N-gram（字符片段）**。

假设我们把 `"apple"` 拆成 3-gram：

`"<ap", "app", "ppl", "ple", "le>"` 以及单词本身 `"<apple>"`。

**“apple”的向量 = 所有这些碎片向量的叠加。**

#### 2.3 核心优势：解决 OOV 问题 (Out-Of-Vocabulary)

这是 FastText 最伟大的贡献。

- **场景**：测试集中出现了一个新词 **"applelike"**（像苹果一样的），这个词在训练集中从未出现过。
- **Word2Vec**：报错！或者把它当成 `<UNK>`（未知词），直接放弃治疗。
- **FastText**：虽然没见过 "applelike"，但我认识 `"apple"` 和 `"like"` 啊！

它会把 `"apple"` 的向量和 `"like"` 的向量拼起来，猜出 "applelike" 大概是“像苹果”的意思。

#### 2.4 优缺点

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **解决生僻词 (OOV)**：对拼写错误、形态变化丰富的语言（如英语、德语、俄语）效果极佳。 **轻量级**：在文本分类任务上，速度极快且效果媲美深度网络。 |
| **缺点** | **模型体积大**：因为要存储海量的 n-gram 碎片向量，模型文件通常比 Word2Vec 大很多。 |

![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816814442-0423a8fa-d2aa-4785-85fd-76e5e78539da.png)

------

### 3. 三大模型横向对比

这部分建议做成表格，方便复习记忆：

| **特性**         | **Word2Vec**           | **GloVe**        | **FastText**                |
| ---------------- | ---------------------- | ---------------- | --------------------------- |
| **核心机制**     | 预测上下文 (Skip-gram) | 统计共现矩阵     | 预测上下文 + 子词 (N-gram)  |
| **最小单位**     | 词 (Word)              | 词 (Word)        | **字符片段 (Subword)**      |
| **能否处理新词** | ❌ 不能 (OOV问题)       | ❌ 不能           | **✅** **能 (基于字根推断)** |
| **训练速度**     | 中等                   | 快               | 较慢 (但在分类任务上极快)   |
| **典型应用**     | 通用 NLP 任务          | 捕捉词语类比关系 | **文本分类、处理拼写错误**  |

------

### 4. Python 代码示例

```python
from gensim.models import FastText

# 1. 准备简单语料
sentences = [["apple", "is", "fruit"], ["banana", "is", "yellow"]]

# 2. 训练 FastText 模型
# min_n=3, max_n=6 表示我们会把单词拆成 3到6 个字符的片段
model = FastText(sentences, vector_size=10, window=3, min_count=1, min_n=3, max_n=6)

# 3. 见证奇迹时刻：查询一个不存在的词 "apple-like"
oov_word = "apple-like"

# 在 Word2Vec 中，这行代码会报错
# 但在 FastText 中，它能算出来！
if oov_word in model.wv:
    print(f"'{oov_word}' 的向量:", model.wv[oov_word])
else:
    # 实际上 Gensim 的 FastText 实现会自动处理，这里只是演示逻辑
    print("FastText 也能根据 n-gram 估算出向量")

# 4. 获取相似词
print(model.wv.most_similar("apple"))
```

**总结**：

- **GloVe** 赢在全局统计。
- **FastText** 赢在细节处理（子词），是目前**如果不使用 BERT 等大模型外，最好用的静态词向量工具**。

------

## 1. 词向量评估🌟

### 词向量评估方式

**核心摘要**：训练词向量就像培养一个学生。

我们有两种方法来测试他的水平：

1. **内在评估**：让他做**“卷子”**（做填空题、找同义词），看基础知识牢不牢。
2. **外在评估**：让他去**“实习”**（做具体项目），看能不能解决实际问题。

#### 1. 内在评估 —— “做卷子”

内在评估不考虑具体的应用场景，直接对词向量本身的质量进行检验。它的核心标准是：**向量空间中的距离，是否符合人类的语义认知？**

##### 1.1 词语类比

这是 Word2Vec 最著名的评估方式，也是检验模型是否学会“逻辑推理”的标准。

- **题目形式**：![img](https://cdn.nlark.com/yuque/__latex/de951302f41d4707b9d80ca1af34dd0f.svg) 之于 ![img](https://cdn.nlark.com/yuque/__latex/54f5fb1b07a88521e7b036e3bc7a5e33.svg)，相当于 ![img](https://cdn.nlark.com/yuque/__latex/a42a4fc28b384cc408de066beed57485.svg) 之于 ![img](https://cdn.nlark.com/yuque/__latex/558270b7f0a90c3c286b860273d106a0.svg) (![img](https://cdn.nlark.com/yuque/__latex/6640eac5b9c30a84e3177517df1952ac.svg))。

- - *例子*：“北京”之于“中国”，相当于“巴黎”之于“**__**”？（答案应该是法国）

- **数学原理**：

模型通过向量加减法来预测 ![img](https://cdn.nlark.com/yuque/__latex/558270b7f0a90c3c286b860273d106a0.svg)：

![img](https://cdn.nlark.com/yuque/__latex/576f590fff900262f702ed9bb4632b09.svg)

也就是：![img](https://cdn.nlark.com/yuque/__latex/06f1740a209cbf9230354e2eeb9a7a1e.svg)。

- **评估标准**：

模型计算出的向量，会在词表中寻找最接近的词。如果 Top 1 是“法国”，则得 1 分；如果是“伦敦”，则得 0 分。最后计算**准确率 (Accuracy)**。

- **常用数据集**：

- - **Google Analogy Dataset**：包含 19,544 个类比问题（既有语义类比，也有语法类比）。

##### 1.2 词语相似度

这是检验模型是否理解“近义词”的标准。

- **测试流程**：

1. 1. **人类打分**：准备一组词对，比如 `(汽车, 轿车)`。找 10 个普通人给它们打分（0-10分）。假设人类平均分是 **9.0**。
   2. **模型打分**：计算这两个词向量的**余弦相似度**。假设模型算出 **0.85**。
   3. **对比排名**：我们不看绝对数值，而是看**排名**。

- - - 人类认为：`相似度(汽车, 轿车) > 相似度(汽车, 自行车)`。
    - 模型也应该认为：`Score(汽车, 轿车) > Score(汽车, 自行车)`。

- **量化指标**：**斯皮尔曼相关系数**

- - 系数 = 1：模型的判断和人类完全一致（完美）。
  - 系数 = 0：模型的判断和人类毫无关系（瞎猜）。

- **常用数据集**：

- - **WordSim-353**：经典的 353 对词语相似度评分集。

##### 1.3 优缺点分析

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **速度快**：不需要训练复杂的下游模型，几秒钟就能跑完评估。 **针对性强**：能直接反映向量的几何质量（是否捕获了语义）。 |
| **缺点** | **与实战脱节**：**内在分数高** ![img](https://cdn.nlark.com/yuque/__latex/762aa5611915d59b5943ec36e2f4b145.svg) **实战效果好**。有时候模型在类比任务上拿了满分，但在情感分析任务里却表现平平。 |

------

#### 2. 外在评估 —— “去实习”

外在评估是指把训练好的词向量，作为**输入特征**（Input Features），喂给一个实际的 NLP 任务（如文本分类、命名实体识别），看最终任务的准确率。

##### 2.1 评估流程

1. **固定词向量**：使用训练好的 Word2Vec (Static)。
2. **构建下游模型**：搭建一个简单的 LSTM 或 CNN 分类器。
3. **训练下游任务**：做比如“电影评论情感分析”任务。
4. **看指标**：如果在同样的网络结构下，使用 **词向量 A** 的准确率是 85%，使用 **词向量 B** 的准确率是 90%，那么我们就说 **B 比 A 好**。

##### 2.2 优缺点分析

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **真实可靠**：这是最能说服人的指标，因为我们训练词向量最终就是为了用的。 |
| **缺点** | **耗时耗力**：训练下游任务可能需要几个小时甚至几天。 **变量干扰**：如果效果不好，你不知道是词向量不好，还是下游分类器的结构没设计好（难以归因）。 |

------

#### 3. Python 代码示例

##### 3.1 准备工作

假设你已经有了一个训练好的模型 `model`。

##### 3.2 定性评估：直接看

最直观的方法，让模型列出它认为最相似的词，人眼一看便知。

```python
# 假设 model 是加载好的 Word2Vec 模型
target_word = "孔乙己"

try:
    # 寻找最相似的 Top 5
    similar_words = model.wv.most_similar(target_word, topn=5)

    print(f"和 '{target_word}' 最相似的词：")
    for word, score in similar_words:
        print(f"- {word}: {score:.4f}")

    # 预期好结果：鲁迅, 闰土, 祥林嫂...
    # 预期坏结果：汽车, 苹果, 只有... (毫无逻辑)

except KeyError:
    print(f"词表中没有 '{target_word}' 这个词")
```

##### 3.3 定量评估：做类比题

这是计算“男人 - 国王 + 女人 = ？”的标准代码。

```python
# 题目：国王 - 男人 + 女人 = ?
result = model.wv.most_similar(positive=['国王', '女人'], negative=['男人'], topn=1)

print(f"国王 - 男人 + 女人 ≈ {result[0][0]} (置信度: {result[0][1]:.4f})")
# 理想输出：王后 (Queen)
```

##### 3.4 加载标准数据集评分

```python
# 注：这需要你下载 questions-words.txt 数据集文件。
# 这一步会跑遍几千道题目，算出准确率
# evaluation_result 包含 (准确率, 正确题目, 错误题目)
accuracy = model.wv.evaluate_word_analogies('questions-words.txt')
print(f"Google 类比数据集准确率: {accuracy[0] * 100:.2f}%")
```

------

### 2. 降维可视化

#### 为什么要降维？

如果把词向量比作宇宙中的星星：

- **高维空间 (300D)**：星星的位置非常精确，距离代表语义。
- **我们想看的 (2D)**：我们需要一张星图。
- **难点**：把立体的地球拍扁成平面的地图，必然会产生变形（比如格陵兰岛在地图上看起来比实际大）。降维算法就是要在“变形”中寻找平衡。

#### 方法一：PCA (主成分分析) —— “投影大师”

##### 原理

PCA (Principal Component Analysis) 是一种**线性降维**技术。

它的核心思想是**“找个最好的角度拍照”**。

- **通俗比喻**：

想象一直茶杯悬浮在空中。

- - 如果你从杯口正上方拍（投影），只看到一个圆圈（丢失了高度信息）。
  - 如果你从侧面拍，能看到杯子的形状。
  - PCA 就是自动计算出“侧面”是保留信息最多的角度，然后把物体投影到这个平面上。

##### 优缺点分析

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **运行极快**：纯数学矩阵运算，几万个词也能瞬间算完。 **保留全局结构**：如果两个词在大方向上离得远，PCA 降维后通常也离得远。 |
| **缺点** | **丢失细节**：它是线性的，对于像“瑞士卷”那样卷曲的复杂数据结构，PCA 会把不同层的点压扁在一起，导致原本不相似的词混淆。 |

![img](https://cdn.nlark.com/yuque/0/2026/png/35927690/1770816828070-e5307659-a153-403b-b7c6-41ee1f6c7481.png)

------

#### 方法二：t-SNE (t-分布邻域嵌入) —— “拓扑魔术师”

这是 NLP 领域**最流行、效果最好**的可视化算法。

##### 原理

t-SNE (t-Distributed Stochastic Neighbor Embedding) 是一种**非线性降维**技术。

它的核心思想是**“只在乎邻居”**。

- **逻辑**：

t-SNE 不在乎“北京”和“华盛顿”离得有多远，它只在乎“北京”和“中国”是不是贴在一起。

它会计算高维空间中每对点的**概率分布**，然后在 2 维空间里模拟这个分布。

- **通俗比喻**：

**把弄皱的纸团铺平**。

想象一张画满了图案的纸被揉成了一团（高维数据）。

- - **PCA** 是直接把纸团压扁，上面的图案全重叠了。
  - **t-SNE** 是小心翼翼地把纸团展开、抚平。虽然纸张边缘可能会变形，但原本画在一起的图案（聚类），现在依然在一起。

##### 优缺点分析

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **聚类效果极佳**：能清晰地把“动物”、“国家”、“动词”分成不同的像岛屿一样的簇 (Clusters)。视觉效果非常震撼。 |
| **缺点** | **慢**：计算量巨大，跑一次可能要好几分钟。 **不稳定**：每次跑出来的形状可能都不太一样（随机性），且并不保留全局距离（簇与簇之间的距离没有明确物理意义）。 |

------

#### Python 代码示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# 1. 准备数据：假设这是模型训练好的高维向量 (4维模拟)
# 我们设计了三类明显的词：水果、IT大厂、家庭角色
words = [
    "苹果", "香蕉", "梨子",       # 水果类
    "谷歌", "微软", "脸书",       # IT类
    "爸爸", "妈妈", "爷爷"        # 家庭类
]

# 模拟的高维向量 (注意观察数值规律)
vectors = np.array([
    [0.1, 0.2, 0.9, 0.1], [0.1, 0.25, 0.85, 0.1], [0.12, 0.22, 0.88, 0.1],
    [0.8, 0.9, 0.1, 0.1], [0.85, 0.92, 0.1, 0.1], [0.82, 0.88, 0.15, 0.12],
    [0.1, 0.1, 0.1, 0.9], [0.12, 0.12, 0.1, 0.88], [0.11, 0.15, 0.12, 0.85]
])

# 2. 使用 t-SNE 降维 (关键步骤)
# n_components=2 表示降维到 2D 平面
# perplexity=2 是一个调节参数，数据量少时设小，数据量大时通常设 30-50
tsne = TSNE(n_components=2, random_state=42, perplexity=2)
vectors_2d = tsne.fit_transform(vectors)

# 3. 绘图配置
plt.figure(figsize=(8, 6))
plt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False   # 用来正常显示负号

# 4. 画散点和标签
x = vectors_2d[:, 0]
y = vectors_2d[:, 1]

plt.scatter(x, y, c='blue', alpha=0.6)

for i, word in enumerate(words):
    # xy是点的坐标，xytext是文字偏移量
    plt.annotate(word, xy=(x[i], y[i]), xytext=(5, 2), 
                 textcoords='offset points', fontsize=12)

plt.title("词向量 t-SNE 可视化效果图")
plt.grid(True, linestyle='--', alpha=0.3)
plt.show()

# 【预期结果】
# 您会看到图上有三个明显分开的小圆圈（簇）：
# 苹果、香蕉、梨子 聚在一起；
# 谷歌、微软、脸书 聚在一起；
# 爸爸、妈妈、爷爷 聚在一起。
```

------

### 总结 

#### 核心逻辑图

- **训练模型** (Word2Vec/BERT) ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **得到向量** ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **看不懂怎么办？**

- - ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **想量化打分？** ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **评估 (Evaluation)**
  - ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **想亲眼看看？** ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **可视化 (Visualization)**

#### 方法对比清单

| **类别**   | **子方法**                     | **核心作用**                          | **优缺点一句话总结**                               |
| ---------- | ------------------------------ | ------------------------------------- | -------------------------------------------------- |
| **评估**   | **内在评估** (词语类比/相似度) | 让模型做填空题。 例：男人-国王+女人=? | **快，但不代表实战强**。适合调参初期。             |
| **评估**   | **外在评估** (下游任务)        | 放到分类/情感分析任务里跑。           | **慢，但最真实**。决定模型能否上线。               |
| **可视化** | **PCA** (主成分分析)           | 线性投影，找最大方差方向。            | **保留全局，丢失局部**。适合看大轮廓。             |
| **可视化** | **t-SNE** (t-分布邻域嵌入)     | 非线性展开，保持邻居关系。            | **保留局部，计算慢**。NLP 画图首选，聚类效果最美。 |

## 句向量生成🌟

### 1. 为什么需要句向量？

在前面的章节中，我们已经学会了如何把“苹果”、“电脑”这样的**单个词**变成向量。但在实际的 NLP 应用中，词向量往往是不够用的。

#### 从“砖块”到“墙壁”

如果把词向量比作**砖块**，那么句向量就是用砖块砌成的**墙壁**。

- **现实需求**：大多数 NLP 任务处理的单位不是单个词，而是**句子**、**段落**甚至**篇章**。

- - *搜索*：用户输入的是一句话（如“电脑开不了机怎么办”）。
  - *推荐*：系统分析的是一篇文章的内容。
  - *情感分析*：我们需要判断整条评论（“这电影虽然特效好，但剧情太烂了”）是褒义还是贬义。

- **核心挑战**：

- - 词的数量是不定的（有的句子 5 个词，有的 50 个词）。
  - 计算机模型（如分类器）通常需要**固定长度**的输入。
  - **目标**：我们需要一种方法，把不定长的句子压缩成一个**固定维度**（比如 300 维）的向量，且这个向量要能代表整句话的核心含义。

### 2. 基于统计的句向量生成

#### 平均池化 (Mean Pooling) —— 简单粗暴的基准线

##### 定义与直觉

**定义**：

平均池化是将句子中所有单词的词向量进行**算术平均**，从而得到一个代表整句话的向量。

**通俗直觉**：

想象在一个二维平面上：

- “苹果”在左上角。
- “香蕉”在右上角。
- 那么“苹果和香蕉”这个句子的向量，就是这两个点连线的**中点（重心）**。

它是所有词在语义空间中的“中心位置”。

##### 数学原理

假设句子 ![img](https://cdn.nlark.com/yuque/__latex/55fc237afbe535f7d8434985b848a6a7.svg) 包含 ![img](https://cdn.nlark.com/yuque/__latex/459f3c80a50b7be28751b0869ef5386a.svg) 个词：![img](https://cdn.nlark.com/yuque/__latex/0d621f9aee0d3b8f1870f33a9b4568c5.svg)。

每个词对应一个 ![img](https://cdn.nlark.com/yuque/__latex/56c1b0cb7a48ccf9520b0adb3c8cb2e8.svg) 维的向量 ![img](https://cdn.nlark.com/yuque/__latex/2ad38a23b40c4d62ec987290b8c64cac.svg)。

句向量 ![img](https://cdn.nlark.com/yuque/__latex/0ff0c5a2aaf82e09d3e460f1dd9830a0.svg) 的计算公式为：

![img](https://cdn.nlark.com/yuque/__latex/37a1ebfee3f2ca227f268bcc211e5255.svg)

也就是：

![img](https://cdn.nlark.com/yuque/__latex/083287dc7bc2a5a006fe5552884c3315.svg)

##### 优缺点深度剖析

| **维度**       | **详细说明**                                                 |
| -------------- | ------------------------------------------------------------ |
| **优点**       | **1. 极速推理**：不需要加载庞大的 BERT 模型，只要查表（词向量矩阵）做加法，几毫秒就能处理一篇文章。 **2. 长度无关**：无论句子是 5 个词还是 500 个词，最后都能变成一样长度的向量（比如 300 维），方便后续输入给分类器。 |
| **致命缺点 1** | **词序丢失 (Bag of Words)**：这是面试必问痛点。 句子 A：“**狗** 咬 **人**” 句子 B：“**人** 咬 **狗**” 因为包含的词汇完全一样，**它们的句向量是 100% 相同的**。模型无法区分谁是主语，谁是宾语。 |
| **致命缺点 2** | **特征稀释 (Feature Dilution)**： 句子：“**虽然** 这个 **的** 价格 **是** 很 **高**，**但是** 质量 **非常****差**。” 核心词是“质量、差”。但句子里充斥着“虽然、的、是”这些无意义的停用词。求平均后，核心词的特征被这些废话“拉平”了，导致句向量变得平庸，情感倾向不明显。 |

------

#### TF-IDF 加权平均 (Weighted Averaging) —— 赋予词语“特权”

##### 为什么需要加权？

为了解决平均池化中的 **“特征稀释”** 问题。

在平均池化里，词语也是搞“一人一票”的民主制，“的”字和“暴跌”拥有同样的投票权，这显然不合理。

我们需要一种机制，让**信息量大**的词（关键词）说话声音大一点，让**废话**（停用词）闭嘴。

##### 核心原理：TF-IDF

我们利用 **TF-IDF** 算法来给每个词打分，作为它的权重（Weight）。

- **TF (Term Frequency)**：词频。这个词在句子里出现的次数。
- **IDF (Inverse Document Frequency)**：逆文档频率。**最关键的指标**。

- - 如果一个词在所有文章里都出现（如“的”、“是”），它的 IDF 很低（接近 0），权重被压低。
  - 如果一个词很少见（如“量子纠缠”、“埃隆马斯克”），它的 IDF 很高，权重被放大。

**加权公式**：

![img](https://cdn.nlark.com/yuque/__latex/fce2fc76101bcbb7ab8cae9709565811.svg)

##### 效果对比

假设我们要计算句子：“**股市****出现****罕见****崩盘**” 的向量。

- **普通平均**：

![img](https://cdn.nlark.com/yuque/__latex/7970e019fa55a2253208823a5ed70003.svg)

*(“出现”这种普通词抢占了 1/4 的权重)*

- **TF-IDF 加权** (假设权重计算后)：

![img](https://cdn.nlark.com/yuque/__latex/30d7a10befc0323feb3baa3c6b937f1e.svg)

*(句向量的方向被强行拉向了“崩盘”和“股市”，语义更加精准)*

##### 优缺点分析

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **去噪能力强**：自动降低停用词干扰，无需人工维护复杂的停用词表。 **保留核心语义**：即使句子很长，只要关键词权重够高，句向量依然能抓住重点。 |
| **缺点** | **依然无序**：加权只是改变了大小，没改变位置。它依然无法区分“猫咬狗”和“狗咬猫”。 |

------

#### Python 代码示例

这段代码非常直观地展示了“普通平均”和“加权平均”的区别。

```python
import numpy as np

# 1. 模拟词向量 (3维)
# "的" 是停用词，向量杂乱；"AI" 和 "未来" 是核心词
vocab = {
    "AI":    np.array([0.9, 0.9, 0.1]), 
    "改变":  np.array([0.5, 0.5, 0.5]),
    "未来":  np.array([0.8, 0.8, 0.2]),
    "的":    np.array([0.1, 0.0, 0.1]) 
}

# 2. 模拟 TF-IDF 权重 (假设这是算出来的结果)
# 注意："的" 权重极低，"AI" 权重很高
idf_weights = {
    "AI":    0.8,
    "改变":  0.5,
    "未来":  0.7,
    "的":    0.01 
}

sentence = ["AI", "改变", "的", "未来"]

# --- 方法一：普通平均池化 ---
def mean_pooling(words):
    vecs = [vocab[w] for w in words]
    return np.mean(vecs, axis=0)

# --- 方法二：加权平均 ---
def weighted_pooling(words):
    vecs = np.array([vocab[w] for w in words])
    weights = np.array([idf_weights[w] for w in words])

    # 核心公式：(权重 * 向量) / 总权重
    # reshape(-1, 1) 是为了让权重能乘到向量的每一维上
    weighted_sum = np.sum(vecs * weights.reshape(-1, 1), axis=0)
    return weighted_sum / np.sum(weights)

# 3. 结果对比
vec_mean = mean_pooling(sentence)
vec_weight = weighted_pooling(sentence)

print(f"普通平均向量: {vec_mean}")
# 结果会被 "的" 字拉低数值

print(f"加权平均向量: {vec_weight}")
# 结果更接近 [0.8, 0.8, ...]，即更接近 "AI" 和 "未来" 的特征
```

------

### 3. 基于模型的句向量生成

**核心摘要**：统计方法（如平均池化）最大的死穴是**“无视语序”**。

为了解决这个问题，我们需要更聪明的模型，它们不仅看词，还能看懂词的前后关系。

#### Doc2Vec —— Word2Vec 的进阶

##### 定义与直觉

**定义**：

由 Word2Vec 的作者 Mikolov 在 2014 年提出。既然我们可以训练“词”的向量，为什么不能直接训练“段落（Paragraph）”的向量呢？

**通俗直觉**：

想象我们在训练 Word2Vec 时，给每个句子发了一个**唯一的身份证号（ID）**。

在训练过程中，模型不仅要记住“猫”后面接“吃鱼”，还要记住“ID:1001 这个句子”的主题是关于“宠物”的。

训练结束后，这个 ID 对应的向量，就是句向量。

##### 核心原理：两种架构。

**(1) PV-DM (Distributed Memory) —— 类似 CBOW**

- **原理**：**“记忆力模式”**。
- **输入**：`[段落向量]` + `[上下文词向量]`
- **任务**：预测 `[中心词]`。
- **机制**：模型认为，要猜出当前这个词，不仅要看前后的词（上下文），还要结合整句话的主题（段落向量）。
- **特点**：效果通常更好，因为利用了上下文信息。

**(2) PV-DBOW (Distributed Bag of Words) —— 类似 Skip-gram**

- **原理**：**“联想模式”**。
- **输入**：`[段落向量]`
- **任务**：预测 `[段落里的词]`。
- **机制**：给一个句子 ID，让模型随机抽段落里的词。这就强迫段落向量必须包含足够丰富的信息（比如“科技”、“金融”），才能猜对里面的词。
- **特点**：训练速度快，但效果略逊于 PV-DM。

##### 优缺点深度剖析

| **维度**     | **详细说明**                                                 |
| ------------ | ------------------------------------------------------------ |
| **优点**     | **1. 考虑语序**：(PV-DM模式) 能够捕捉词与词之间的顺序关系，不再是“词袋模型”。 **2. 擅长长文本**：非常适合处理**篇章级**的任务（如新闻分类、论文查重），能很好地抓住整体主题。 |
| **致命缺点** | **推理门槛高**：这是它最麻烦的地方。 对于 Word2Vec，来个新词查表就行。 对于 Doc2Vec，来个新句子（测试集），**它没有 ID**。你必须把这个新句子**冻结**网络参数，重新跑几十次梯度下降（Inference Stage），算出它的向量。这在实时系统中非常慢。 |

------

#### BERT 的 [CLS] 标记 —— 动态语义的巅峰

##### 为什么需要动态向量？

这是 Word2Vec/Doc2Vec 时代的痛点：**多义词（Polysemy）**。

- **静态模型**：词表里，“苹果”永远只有 1 个向量。无论是在“**苹果**很好吃”还是“**苹果**股价大跌”里，它的向量是一模一样的。
- **动态模型 (BERT)**：它没有固定的词向量表。它是一个**函数** ![img](https://cdn.nlark.com/yuque/__latex/49bd7a79bdc2aed7b017c9863504e35a.svg)。它会根据上下文，现场算出“苹果”的含义。

##### 核心原理：注意力机制与 [CLS] (Mechanism)

BERT (Bidirectional Encoder Representations from Transformers) 是目前工业界的**绝对主流**。

- **特殊的 [CLS] 标记**：

BERT 强制要求：所有句子输入前，必须在开头加一个特殊符号 `**[CLS]**` (Classification)。

- - 输入：`[CLS] 我 喜欢 NLP [SEP]`

- **Self-Attention (自注意力机制)**：

在 12 层神经网络的每一层中，`[CLS]` 都会和句子里的每一个词（我、喜欢、NLP）进行“信息交换”。

- - 它会吸收“喜欢”的情感特征。
  - 它会吸收“NLP”的技术特征。

- **最终输出**：

到了最后一层，`[CLS]` 位置的那个 768 维向量，就已经**浓缩**了整句话的精华。我们直接把它拿出来，就是句向量。

##### 优缺点深度剖析

| **维度** | **详细说明**                                                 |
| -------- | ------------------------------------------------------------ |
| **优点** | **1. 语境感知**：这是降维打击。它能精准区分“Bank (河岸)”和“Bank (银行)”。 **2. 深度理解**：能处理复杂的否定句、倒装句（如“我不认为这不好”），这是统计模型做不到的。 |
| **缺点** | **1. 算力黑洞**：BERT 模型巨大，推理速度比 Word2Vec 慢几十倍甚至上百倍。 **2. 各向异性 (Anisotropy)**：这是一个进阶坑点。原生 BERT 的 `[CLS]` 向量在空间中分布不均匀（都挤在一个狭窄的锥形区域），导致计算余弦相似度时，不管什么句子，分数都偏高（比如全是 0.8 以上）。通常需要配合 **SimCSE** 或 **BERT-whitening** 等技术来校正。 |

------

#### Python 代码示例

使用 `sentence-transformers` 库（工业界最常用的 BERT 封装库）来生成高质量句向量。

```python
# 首先需要安装: pip install sentence-transformers
from sentence_transformers import SentenceTransformer

# 1. 加载预训练模型
# 'all-MiniLM-L6-v2' 是一个速度快且效果好的轻量级 BERT 模型
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. 准备数据
sentences = [
    "This is an example sentence",    # 句子 1
    "Each sentence is converted"      # 句子 2
]

# 3. 生成句向量 (Encoding)
embeddings = model.encode(sentences)

# 4. 查看结果
print(f"句子数量: {len(embeddings)}")
print(f"向量维度: {embeddings[0].shape}") # 通常是 384 或 768 维

# 5. 打印前 5 位看看
print(f"句子 1 的向量前 5 位: {embeddings[0][:5]}")

# 这种方式生成的向量，已经完美处理了语序和多义词问题。
```

------

### 4. 四种方法总结对比

| **方法**        | **核心技术**       | **语序理解** | **速度** | **精度** | **推荐场景**                                |
| --------------- | ------------------ | ------------ | -------- | -------- | ------------------------------------------- |
| **平均池化**    | 词向量求和除以 N   | ❌ 无         | 🚀 极快   | ⭐⭐       | 简单的基准测试、对速度要求极高的初筛        |
| **TF-IDF 加权** | 关键词加权求和     | ❌ 无         | 🚀 极快   | ⭐⭐⭐      | 短文本关键词匹配、关键词提取                |
| **Doc2Vec**     | 训练段落 ID        | ✅ 有         | 🐢 较慢   | ⭐⭐⭐      | 长文档分类、博客/新闻查重                   |
| **BERT [CLS]**  | Transformer 注意力 | ✅ 有         | 🐌 慢     | ⭐⭐⭐⭐⭐    | **语义搜索、智能问答、情感分析** (当前首选) |

------

## 文本相似度计算

**核心摘要**：把文本变成了向量（坐标）之后，计算“语义相似度”就变成了计算初中几何里的“距离”或“角度”。

本节将深入对比 NLP 领域的“黄金标准”——余弦相似度，以及它与欧氏距离的区别。

### 1. 核心问题：如何定义“像”

在数学空间里，两个点靠得越“近”，代表它们的含义越相似。

但是，“近”的定义通常有两种直觉：

1. **方向一致**（夹角小）？ ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **余弦相似度** (NLP 首选)
2. **物理距离近**（直线距离短）？ ![img](https://cdn.nlark.com/yuque/__latex/33b44e34aa35b8c4ecd0606453ee68e9.svg) **欧氏距离**

------

### 2. 方法一：余弦相似度 (Cosine Similarity) —— NLP 的黄金标准

#### 2.1 定义

通过计算两个向量在多维空间中**夹角的余弦值**来衡量它们的相似度。

#### 2.2 原理 

它只关心**方向**，不关心**长度**。

- **公式**：

![img](https://cdn.nlark.com/yuque/__latex/29de8209aa479d66b2d97c810be63cac.svg)

*(分子是点积，分母是模长的乘积)*

- **取值范围**：`[-1, 1]`

- - **1**：方向完全一致（夹角 ![img](https://cdn.nlark.com/yuque/__latex/f1c3deea7e00ba063d1c7d0a8702aae3.svg)）。语义完全相同。
  - **0**：方向垂直（夹角 ![img](https://cdn.nlark.com/yuque/__latex/3f335a7d9478145b7eba9371ca128373.svg)）。语义毫无关系（正交）。
  - **-1**：方向完全相反（夹角 ![img](https://cdn.nlark.com/yuque/__latex/bddcb31928d83123df60d370832b885a.svg)）。语义完全对立。

#### 2.3 为什么它是首选？

- **场景**：一篇短新闻（出现 1 次“苹果”）和一篇长论文（出现 100 次“苹果”）。
- **分析**：

- - 它们的向量**方向**是一致的（都是关于苹果）。
  - 但向量**长度**差了 100 倍。

- **结果**：余弦相似度会把分母（长度）除掉，只看方向，所以能判定它们高度相似。

#### 2.4 优缺点分析

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **鲁棒性强**：不受文本长度影响（对长短文本匹配非常友好）。 **标准化**：结果固定在 -1 到 1 之间，方便设定阈值（比如 >0.8 算抄袭）。 |
| **缺点** | **丢失幅度信息**：如果你关注的是“强度”（比如用户打分 5 分 vs 1 分），它可能无法区分（如果向量方向一致的话）。 |

------

### 3. 方法二：欧氏距离 (Euclidean Distance) —— 直觉的陷阱

#### 3.1 定义

这就是我们在二维平面上用尺子量出来的**直线距离**。

#### 3.2 原理

计算两个点之间的物理距离。

- **公式**：

![img](https://cdn.nlark.com/yuque/__latex/ddaa8a3096c684b629d34698719c4f82.svg)

- **关系**：距离越小，相似度越高。

#### 3.3 优缺点分析

| **维度** | **说明**                                                     |
| -------- | ------------------------------------------------------------ |
| **优点** | **符合直觉**：在低维空间（如 2D 地图）中，这是最自然的距离定义。 |
| **缺点** | **对长度敏感**：这是致命伤。如前所述，长文本的向量模长很大，短文本很小。哪怕内容一样，欧氏距离也会判定它们“相距甚远”。 **维度灾难**：在高维空间（如 768 维）中，欧氏距离的区分度会变差（所有点之间的距离都差不多远）。 |

------

### 4. 实战应用场景

#### 4.1 论文查重 / 版权检测

- **流程**：将文章切分成句子，生成句向量。
- **计算**：计算与数据库中文章的**余弦相似度**。
- **判定**：如果某段文字相似度 > 0.85，标红警告。

#### 4.2 推荐系统 (Recommendation)

- **用户向量**：根据用户历史浏览记录生成。
- **新闻向量**：根据新闻内容生成。
- **推荐逻辑**：计算 Cosine(用户, 新闻)，把分数最高的推给他。

#### 4.3 智能客服 (QA Bot)

- **用户问**：“怎么退款？”
- **知识库**：“退货流程说明”、“如何申请退款”、“付款方式”。
- **匹配**：计算相似度，发现“如何申请退款”与用户问题向量最近，直接返回答案。

------

### 5. Python 代码示例

使用 `sklearn` 库计算余弦相似度是最标准的做法。

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 1. 假设这是两个句子的向量 (比如由 BERT 生成的)
# 向量 A: [1, 1] (方向 45度)
# 向量 B: [2, 2] (方向 45度，但长度是 A 的两倍)
# 向量 C: [1, 0] (方向 0度)
vec_a = np.array([[1, 1]])
vec_b = np.array([[2, 2]])
vec_c = np.array([[1, 0]])

# 2. 计算余弦相似度
# 注意：输入必须是二维数组
score_ab = cosine_similarity(vec_a, vec_b)
score_ac = cosine_similarity(vec_a, vec_c)

print(f"A 和 B (长度不同，方向相同): {score_ab[0][0]:.4f}")
# 结果: 1.0000 (完美相似，验证了余弦相似度忽略长度)

print(f"A 和 C (方向不同): {score_ac[0][0]:.4f}")
# 结果: 0.7071 (即 cos(45度))

# 3. 欧氏距离对比
dist_ab = np.linalg.norm(vec_a - vec_b)
print(f"A 和 B 的欧氏距离: {dist_ab:.4f}")
# 结果: 1.414 (认为不相似，这就是欧氏距离在 NLP 中的坑)
```

------

### 6. 总结对比 

| **方法**       | **核心逻辑**      | **适用场景**                                   | **关键缺点**           |
| -------------- | ----------------- | ---------------------------------------------- | ---------------------- |
| **余弦相似度** | **看方向** (夹角) | **绝大多数 NLP 任务** (文本分类、推荐、搜索)   | 忽略了数值大小（强度） |
| **欧氏距离**   | **看距离** (直线) | 某些聚类任务 (K-Means)，或特征数值有物理意义时 | 对文本长度太敏感       |