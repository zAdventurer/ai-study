# 词向量与句向量

## 第一部分：引言与学习目标

### 1. 核心问题：计算机如何理解“人话”？

> **根本矛盾**：计算机只认识数字（0和1），而人类语言是高度抽象的符号系统。

NLP（自然语言处理）的核心任务，就是搭建一座桥梁，把**离散的符号（Text）** 映射为 **连续的数值（Vector）**。

- **传统做法（符号匹配）**：类似 Word 里的 `Ctrl+F`。搜索“开心”，计算机只会找这两个字，完全不知道“快乐”、“愉悦”和它是一个意思。
- **现代做法（语义理解）**：通过向量技术，将词语映射到数学空间。在这个空间里，“开心”和“快乐”的坐标距离非常近。

### 2. NLP 技术演进的三大里程碑

理解这个历史，你才能明白为什么 Word2Vec 是革命性的。

#### **第一阶段：规则系统 (Rule-Based System) —— 1950s**

- **核心逻辑**：**硬编码 (Hard Coding)**。
- **实现方式**：语言学家总结出成千上万条语法规则，程序员把它们写成复杂的 `if-else` 代码。
  - *例子*：`if "主语" + "be动词" + "not": return "否定句"`
- **致命缺陷**：语言太灵活了，规则永远写不完。一旦遇到网络新词或不规范的口语，系统立刻崩溃。

#### **第二阶段：统计学习 (Statistical Learning) —— 2000s**

- **核心逻辑**：**数据驱动 (Data Driven)**。不再人工写规则，而是让机器从海量文本中统计概率。
- **代表技术**：词袋模型 (Bag-of-Words)、独热编码 (One-Hot)。
- **数据结构痛点**：
  - 此时的代码逻辑通常是构建一个**稀疏矩阵 (Sparse Matrix)**。
  - 如果你有 10 万个词，每个词就是一个长度 10 万的向量，其中 99999 个位置是 0，只有 1 个位置是 1。
  - **后果**：这就是我们在代码 `01` 中看到的**维度灾难 (Curse of Dimensionality)** —— 内存占用极大，且无法计算词与词之间的相似度。

#### **第三阶段：深度学习 (Deep Learning) —— 2010s+**

- **核心逻辑**：**分布式表示 (Distributed Representation)**。
- **技术突破**：神经网络 (Neural Networks)。
- **核心差异**：
  - 从“高维稀疏”变成了**“低维稠密”**。
  - 不再用 10 万维来表示一个词，而是压缩到 50维 或 300维 的实数向量（如 `[0.21, -0.98, 0.55...]`）。
  - 这就是本章的主角 **Word2Vec** 和后续的 **BERT**。

### 3. 本章核心学习目标（深度版）

我们不仅要会调库，更要掌握以下底层逻辑：

1. **数据结构的质变**：
   - 深刻理解为什么从 **One-Hot (独热)** 转向 **Embedding (嵌入)** 是必然趋势。（代码 `01` vs `02` 的本质区别）
2. **数学原理的直觉**：
   - 理解 **余弦相似度** 的几何意义：为什么在 NLP 中，看“方向”比看“距离”更重要？
3. **模型架构的权衡**：
   - 掌握 **Word2Vec** 的两种训练模式（CBOW vs Skip-gram）的适用场景。
   - 理解 **GloVe**（全局统计）和 **FastText**（子词特征）是如何修补 Word2Vec 的缺陷的。

- **经典案例**：`King - Man + Woman = Queen`（计算机能算出国王减去男人加上女人，结果等于皇后）。

## 词向量基础与独热编码 (Deep Dive)

### 1. 独热编码 (One-Hot Encoding)：计算机的“直觉”

这是 NLP 最早期的解决方案。它的逻辑非常符合计算机的二进制直觉，但完全违背了语言学的逻辑。

- **底层逻辑 (Code Logic)**：
  - 建立一个词表：`['猫', '狗', '苹果']`。
  - 给每个词分配一个**唯一索引 (Index)**：猫=0，狗=1，苹果=2。
  - **生成向量**：
    1. 创建一个全为 0 的向量，长度等于词表大小。
    2. 只在对应的索引位置，把 0 改成 1。
- **形式**：
  - 猫：`[1, 0, 0]`
  - 狗：`[0, 1, 0]`
  - 苹果：`[0, 0, 1]`

### 2. 独热编码的三大致命缺陷 

为什么现在的大模型不用它了？**代码 `01`** 用数学计算给出了铁证。

#### **(1) 维度灾难 (Dimension Explosion)**

> **代码逻辑揭秘**：请回忆代码中计算 `memory_mb` 的公式。

- **问题**：向量的维度 = 词汇表的大小。
- **计算推导**：
  - 假设中文词库有 **30万** 个词。
  - 每个词都需要一个 **30万维** 的向量。
  - 如果用 `float32` (4字节) 存储，构建一个简单的查找表矩阵，内存消耗 = $300,000 \times 300,000 \times 4 \text{ Bytes} \approx \textbf{335 GB}$。
- **结论**：光是存词向量就把内存撑爆了，根本没法算。

#### **(2) 稀疏性 (Sparsity)**

- **问题**：在这个 30 万维的向量里，只有 **1个** 位置是有效信息（1），其余 **299,999** 个位置全是废话（0）。
- **后果**：计算资源被极度浪费，数据效率极低。

#### **(3) 语义鸿沟 (Semantic Gap)** —— 最本质的缺陷

> **代码逻辑揭秘**：代码演示了 `dot(猫, 狗)` 的结果。

- **数学事实**：
  - $V_{猫} = [1, 0, 0, ...]$
  - $V_{狗} = [0, 1, 0, ...]$
  - **点积 (Dot Product)**：$1\times0 + 0\times1 + 0\times0 ... = \textbf{0}$。
- **几何意义**：
  - 在数学空间里，点积为 0 意味着**相互垂直 (Orthogonal)**。
  - 计算机认为“猫”和“狗”的关系，就像“x轴”和“y轴”一样，**毫无瓜葛**。
  - **这就是语义鸿沟**：无论两个词在现实中多相似，在 One-Hot 空间里它们的距离永远是一样的（$\sqrt{2}$），计算机无法理解“近义词”。

### 3. 破局：分布式表示 (Distributed Representation)

为了解决上述问题，Hinton 等人提出了革命性的思想。

- **核心转换**：
  - **从“高维稀疏”到“低维稠密”**。
  - 不再用“一个位置”代表一个词，而是用**“一串实数”**共同代表一个词。
- **对比**：
  - One-Hot: `[0, 0, 0, 1, 0, ...]` (维度 10万)
  - Embedding: `[0.2, -0.5, 0.8, ...]` (维度 50 或 300)
- **神奇效果**：
  - 在这个新的低维空间里，“猫”和“狗”的向量可能在某些维度上数值非常接近（比如代表“动物属性”的那个维度）。

### 4. 数学基石：距离计算

当词变成了稠密向量，我们终于可以计算“相似度”了。

#### **(1) 欧氏距离 (Euclidean Distance)**

- **定义**：空间中两点的直线距离（尺子量出来的）。
- **公式**：$\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$
- **缺陷**：受向量模长（长度）影响太大。有些词因为频率高或者训练原因，模长特别长，会导致距离计算失真。

#### **(2) 余弦相似度 (Cosine Similarity)** —— NLP 的首选

> **重点**：这是所有向量检索（RAG）、推荐系统的核心算法。

- **定义**：空间中两个向量的**夹角余弦值**。

- **公式**：

  $$\text{Cosine} = \frac{A \cdot B}{\|A\| \times \|B\|}$$

- **直觉理解**：

  - 我们只在乎**方向**是否一致，不在乎**力气**（长度）大小。
  - 只要方向一致（夹角0度），相似度就是 1（满分）。
  - **代码体现**：在后续代码中，我们总是先做归一化（除以模长），其实就是为了消除长度影响，只看方向。

## 第三部分：Word2Vec 深度解析 (核心算法)

### 1. 本质：它不是“深度”学习

虽然 Word2Vec 开启了深度学习在 NLP 的应用，但它本身是一个**浅层神经网络 (Shallow Neural Network)**。

- **结构**：只有三层 —— **输入层 (Input) -> 投影层 (Projection) -> 输出层 (Output)**。
- **目的**：我们训练这个神经网络，**根本不是为了最后的预测结果**（预测下一个词是什么），而是为了**窃取**它中间的那一层权重矩阵（Weight Matrix）。
  - 这个权重矩阵的每一行，就是我们梦寐以求的 **词向量**。

### 2. 两大流派：CBOW vs Skip-gram

在代码中，有一个不起眼的参数 `sg`，它决定了整个模型的训练逻辑。

#### **(1) CBOW (Continuous Bag of Words) —— "完形填空"**

- **逻辑**：已知上下文，预测中间词。
  - *输入*：`["我", "喜欢", "___", "和", "狗"]`
  - *预测*：`"猫"`
- **数学处理**：
  - 它将上下文所有词的向量相加或求平均，压缩成一个向量，然后去预测目标。
- **特点**：
  - **快**：因为不管上下文有多少词，都压缩成一次计算。
  - **平滑**：因为取了平均，容易抹平生僻词的细节。

#### **(2) Skip-gram —— "举一反三" (代码 `02` 采用的模式)**

> **代码逻辑揭秘**：在 `02_Word2Vec训练与应用.py` 第 36 行，我们设置了 `sg=1`。这就是选择了 Skip-gram。

- **逻辑**：已知中间词，预测上下文。
  - *输入*：`"猫"`
  - *预测*：`["我", "喜欢"]` (左边) 和 `["和", "狗"]` (右边)
- **训练过程拆解**：
  - 对于句子 "我 喜欢 猫 和 狗"，窗口大小 `window=2`。
  - 模型会构建 4 组训练数据：`(猫, 我)`, `(猫, 喜欢)`, `(猫, 和)`, `(猫, 狗)`。
  - **它把一对多的问题，拆解成了 4 个一对一的分类问题**。
- **为什么代码里选它？**
  - **数据量小**：我们的演示语料只有 10 句话。Skip-gram 能从一句话里拆出更多的训练样本（数据利用率高）。
  - **生僻词友好**：它不会对输入求平均，每个词都有独立展现的机会。

### 3. 核心参数的物理意义

在 `gensim` 的代码调用中，每一项参数都有其对应的算法原理：

Python

```
# 代码 02 第 36 行
model = Word2Vec(
    sentences, 
    vector_size=50,  # 嵌入维度
    window=3,        # 上下文窗口
    min_count=1,     # 词频截断
    sg=1             # 架构选择
)
```

1. **`vector_size=50` (嵌入维度)**：
   - **含义**：这是神经网络中间投影层的神经元个数。
   - **权衡**：
     - 太小（如 10）：容纳不下复杂的语义，词与词挤在一起。
     - 太大（如 1000）：容易**过拟合 (Overfitting)**，且计算量呈平方级增长。对于中文小语料，50-200 是常规选择。
2. **`window=3` (上下文窗口)**：
   - **含义**：模型“看”多远。
   - **影响**：
     - **小窗口 (2-5)**：学到的是**句法/功能性**关系（如“喜欢”后面常接“动词”）。模型更关注“这个词怎么用”。
     - **大窗口 (5-10)**：学到的是**语义/主题性**关系（如“飞机”和“蓝天”虽然不挨着，但在同一个主题里）。模型更关注“这个词是什么意思”。

### 4. 训练加速的大杀器：负采样 (Negative Sampling)

如果词库有 30 万词，每次预测都要算 30 万个概率（Softmax），这是不可能完成的任务。

- **问题**：计算分母 $\sum e^{score}$ 需要遍历全词表。
- **解决方案**：**负采样**（虽然代码没显式写，但 Gensim 默认开启）。
- **逻辑**：
  - 训练 "猫" -> "鱼" (正样本) 时。
  - 我们不跟全字典比，我们随机抽 5 个**负样本**（比如 "桌子"、"汽车"、"股票"）。
  - **任务变更**：
    - 让 `Prob(鱼|猫)` 接近 1。
    - 让 `Prob(桌子|猫)` 接近 0。
  - **计算量暴跌**：从 30 万次运算降到 6 次运算。

## 第四部分：主流词向量模型演进 (GloVe & FastText)

### 1. GloVe (Global Vectors) —— "站在上帝视角的统计派"

Word2Vec 的一个隐性缺点是：它是“局部”的。

- **Word2Vec 的局限**：它在训练时，永远只盯着当前窗口里的 3-5 个词看（Skip-gram）。它像一个拿着放大镜在书上爬行的蚂蚁，虽然能看清局部关系，但容易忽略整本书的宏观统计信息。
- **GloVe 的反击 (Stanford)**：
  - **核心逻辑**：**全局共现矩阵 (Global Co-occurrence Matrix)**。
  - **原理**：
    1. 在训练神经网络之前，先扫描一遍全书，统计一个巨大的矩阵 $X$。
    2. $X_{ij}$ 表示：词 $i$ 和词 $j$ 在整个语料库中一共共同出现了多少次。
    3. 训练目标是让两个词向量的点积 $V_i \cdot V_j$ 去拟合 $\log(X_{ij})$。
- **数学直觉**：
  - 它强行把“统计学”（Counting）和“预测法”（Prediction）结合在了一起。
  - **杀手锏**：在处理 **线性类比 (Linear Analogy)** 任务上表现极佳。
  - *例子*：计算 `King - Man + Woman = ?`，GloVe 算出的结果通常比 Word2Vec 更接近 `Queen`，因为它利用了全局的共现概率比率。

### 2. FastText —— "拆字解意的实用派"

这是 Facebook AI (FAIR) 针对 Word2Vec 最致命的 **OOV (Out-Of-Vocabulary)** 问题提出的解决方案。

- **Word2Vec 的致命伤**：**最小单位是词**。
  - 字典里有 "apple"，向量是 `[0.1, 0.2]`。
  - 如果测试时遇到了复数 "apples" 或者新词 "applePencil"，只要字典里没有，模型就**完全不认识**，直接报错或标记为 `<UNK>` (Unknown)。
- **FastText 的创新：N-gram 子词特征**
  - **原理**：它不再把一个词看作不可分割的原子，而是看作一群**字符片段 (Character n-grams)** 的集合。
  - **拆解过程**：
    - 假设词是 `"apple"`，设置 $n=3$。
    - 它会把词拆成：`<ap`, `app`, `ppl`, `ple`, `le>`。
    - **最终向量** = 词本身的向量 + 所有子词向量的叠加。
- **代码逻辑推演**：
  - 当遇到生词 `"applePencil"` 时：
    1. Word2Vec: "没见过，查无此人"。
    2. FastText: "虽然没见过整体，但我见过 `app`, `ple`, `Pen`, `cil` 这些片段。我把它们的向量拼凑起来，大概能猜到这是一种苹果相关的笔。"
- **适用场景**：
  - **包含大量复合词的语言**（如德语、土耳其语）。
  - **专业领域**（如生物医药，化学物质名字很长且有词根规律）。
  - **社交网络**（用户喜欢造词，如“绝绝子”、“火钳刘明”）。

### 3. 三大模型硬核对比 

| **维度**     | **Word2Vec (Google)**                            | **GloVe (Stanford)**                     | **FastText (Facebook)**               |
| ------------ | ------------------------------------------------ | ---------------------------------------- | ------------------------------------- |
| **核心架构** | **浅层神经网络** (CBOW/Skip-gram) 预测局部上下文 | **矩阵分解** + 神经网络 拟合全局共现概率 | **Word2Vec + N-gram** 引入子词特征    |
| **训练依据** | 局部滑动窗口                                     | 全局统计信息                             | 局部窗口 + 字符级特征                 |
| **OOV 能力** | ❌ **弱** (遇到生词直接挂)                        | ❌ **弱** (同上)                          | ✅ **强** (能根据词根猜含义)           |
| **训练速度** | 中等                                             | 快 (易于并行)                            | 🚀 **极快** (通常比深度网络快一个量级) |
| **内存消耗** | 低 (只存词向量)                                  | 中 (需构建共现矩阵)                      | ⚠️ **高** (因为要存海量的子词 Hash 表) |
| **工业定位** | **万金油基准 (Baseline)**                        | 适合需要精细**类比推理**的任务           | 适合**文本分类**、**生僻词多**的场景  |

### 4. 共同的“天花板”：静态向量的悲哀

这一章的最后，我们需要意识到这三个模型有一个共同的死穴，这也为下一代模型（BERT）的登场埋下了伏笔。

- **问题：一词多义 (Polysemy)**
  - 语言是复杂的。
  - 场景 A：“我喜欢吃**苹果**。” (Fruit)
  - 场景 B：“**苹果**股价大跌。” (Company)
- **缺陷**：
  - 在 Word2Vec/GloVe/FastText 中，**"苹果" 只有一个固定的向量** `[0.5, 0.1, ...]`。
  - 无论上下文怎么变，它取出来的向量永远是一样的。模型无法区分这到底是个水果还是个公司。
- **结论**：这就是所谓的 **静态词向量 (Static Word Embedding)**。它很好，但还不够智能。现代 NLP 需要的是能随上下文变形的 **动态词向量**。

## 第五部分：词向量评估与可视化 (Evaluation & Visualization)

模型训练只是第一步，**验证**才是决定能否上线的关键。词向量是看不见摸得着的 50 维数组，我们必须通过特殊的手段来“体检”。

### 1. 评估方法：怎么知道模型练得好不好？

> **通俗理解**：就像评价一个学生，既可以看他的“平时作业”，也可以看他的“期末成绩”。

#### **(1) 内在评估 (Intrinsic Evaluation) —— "平时作业"**

- **核心逻辑**：不看应用，直接看向量本身的数学性质是否符合人类直觉。
- **两大测试题**：
  - **词类比 (Word Analogy)**：
    - *考题*：`King - Man + Woman = ?`
    - *预期*：计算出的向量应该和 `Queen` 的向量距离最近。
    - *意义*：验证模型是否学到了**语义的代数逻辑**。
  - **相似度排序 (Similarity Ranking)**：
    - *考题*：让模型给 `(猫, 狗)` 和 `(猫, 汽车)` 打分。
    - *预期*：`Sim(猫, 狗)` 必须远大于 `Sim(猫, 汽车)`。
- **优缺点**：
  - ✅ 计算极快，跑完模型几秒钟就能出结果。
  - ❌ “平时作业”写得好，不代表“期末考试”能考高分。有时词类比很准，但在实际分类任务里效果一般。

#### **(2) 外在评估 (Extrinsic Evaluation) —— "期末成绩"**

- **核心逻辑**：把训练好的词向量，放到真实的下游任务（Downstream Tasks）中去试。
- **代码对应**：
  - 比如放到 **代码 `04_综合实践_文本分析系统.py`** 里的 `LogisticRegression` 分类器中。
- **评判标准**：
  - 如果用了你的词向量，文本分类的准确率 (Accuracy) 从 80% 提升到了 85%，那就是好模型。
- **优缺点**：
  - ✅ **金标准**，工业界唯一认可的指标。
  - ❌ 费时费力，需要搭建一套完整的系统才能测。

### 2. 可视化：把高维压扁看一眼

> **根本矛盾**：人的眼睛只能看懂 2D (平面) 或 3D (立体) 的图像，但词向量通常是 50 维甚至 300 维的。

我们需要使用 **降维算法 (Dimensionality Reduction)**，把高维空间的“星系”投影到二维纸面上。

#### **(1) PCA (主成分分析) —— "上帝视角" (代码 `02` 采用)**

> **代码逻辑揭秘**：请看 `02_Word2Vec训练与应用.py` 第 56 行 `pca.fit_transform(vectors)`。

- **核心原理**：**线性投影**。
  - 想象一个橄榄球（3D物体）。PCA 就是找一个特定的角度打一束光，让投射在地上的影子（2D）面积最大，最能看出它是橄榄球的形状。
  - 它寻找数据方差最大的方向（主成分），保留数据的**整体结构**。
- **特点**：
  - **全局性**：它能让你看清“动物”在左边，“水果”在右边，这种宏观分布。
  - **速度快**：纯线性代数计算，几万个词也能秒出图。

#### **(2) t-SNE —— "显微镜视角" (PPT 重点)**

> **注意**：代码里没用这个，因为它太慢了，但它是论文配图的标配。

- **核心原理**：**流形学习 (Manifold Learning)**。
  - 它不只是简单的投影，而是试图保持**邻居关系**。
  - 如果两个词在高维空间是邻居，t-SNE 会千方百计地让它们在二维平面上也挨在一起；如果离得远，就推得更远。
- **特点**：
  - **局部性**：画出来的图非常惊艳，相似的词会聚成一个个界限分明的“小岛”。
  - **极慢**：计算复杂度极高，通常只能画几千个词。

### 3. 代码里的“陷阱”：为什么图上有些词离得远？

在运行 **代码 `02`** 的 PCA 绘图时，你可能会发现 "猫" 和 "狗" 并没有挨得特别近，这正常吗？

- **原因 1：维度压缩损失**
  - 把 50 维的信息强行压扁到 2 维，必然会丢失大量信息（比如丢失了 48 个维度的特征）。哪怕是 PCA 也无法完美还原。
- **原因 2：数据量太小**
  - 我们的 `sentences` 只有 10 句话。模型还没见过足够多的“猫”和“狗”的共现，所以学出来的向量本身就不够完美。**这是数据的问题，不是算法的问题。**

## 第六部分：句向量生成 (Sentence Embedding)

### 1. 概述与核心挑战

词向量让我们看懂了“单词”，但人类交流的基本单位是“句子”。句向量的目标是：**把长度不一的句子，压缩成一个固定长度的向量**，并且要保留这句话的**核心语义**。

#### **核心挑战 (The Core Challenges)**

1. **输入变长 (Variable Length)**：
   - 有的句子 3 个词，有的 50 个词。如何让它们最终都变成一个 **50维** 的向量？
2. **语序丢失 (Order Information)**：
   - *"狗咬人"* 和 *"人咬狗"*。
   - 如果只是简单的把词加起来，这两句话的向量是一模一样的。如何让模型识别出主谓宾的顺序差异？
3. **一词多义 (Polysemy)**：
   - *"苹果销量不错"* vs *"苹果真甜"*。同一个词在不同句子里贡献的语义应该不同。

------

### 2. 技术演进 I：基于词向量的统计组合 (Baseline)

这是最基础的方法，也是我们 **代码 `03_句向量与文本相似度.py`** 中使用的方法。

#### **(1) 平均池化 (Average Pooling)**

> **代码对应**：`03.py` 中的 `sentence_vector` 函数。

- **原理**：
  - 把句子里所有词的向量加起来，除以词数。
  - $$V_{sent} = \frac{1}{n} \sum V_{word}$$
- **比喻**：**“果汁搅拌法”**。
  - 把“苹果、香蕉、牛奶”扔进搅拌机。
  - **优点**：计算极快，甚至不需要重新训练模型。
  - **缺点**：完全丢失语序（变成了混合果汁，还原不出原来的水果排列顺序）。

#### **(2) TF-IDF 加权平均**

> **纪要重点**：下午课程中提到的优化方案。

- **原理**：
  - 在搅拌之前，给重要的词（如“核聚变”）多加点分量，给不重要的词（如“的/是”）少加点分量。
  - $$V_{sent} = \sum (\text{Weight}_{TF-IDF} \times V_{word})$$
- **改进**：比单纯平均更能抓住句子的重点，但依然无法解决“语序”问题。

------

### 3. 技术演进 II：Doc2Vec (PV-DM/PV-DBOW)

为了解决“平均法”丢失语序的问题，Word2Vec 的作者后来提出了 **Doc2Vec**。

- **核心思想**：给句子发一张**“专属身份证”**。
- **原理 (PV-DM 模型)**：
  - 在训练 Word2Vec 时，除了输入上下文单词，还额外输入一个 **Paragraph ID (段落向量)**。
  - 训练目标：`[段落向量 + 上下文词] --> 预测中心词`。
  - 训练结束后，这个“段落向量”就记住了整个句子的语义和语序信息。
- **优势**：
  - 它不再是词向量的简单叠加，而是真正学到了句子的整体结构。
  - 比平均池化更能区分 *"狗咬人"* 和 *"人咬狗"*。

------

### 4. 技术演进 III：BERT 句向量 (State-of-the-Art)

这是 PPT 中提到的“高级应用与展望”，也是目前工业界的最强方案。

- **痛点打击**：解决**动态语义**。
- **原理**：
  - BERT 模型在处理句子时，会在开头加一个特殊的标记 **`[CLS]`**。
  - 经过 12 层深层网络的复杂的**注意力机制 (Self-Attention)** 交互后。
  - **`[CLS]` 位置的向量** 就浓缩了整句话的精华，且包含了上下文的细微差异。
- **对比**：
  - **Word2Vec 平均**：是“死”的，不管句子怎么变，词向量不变。
  - **BERT**：是“活”的，同一个词在不同语境下权重自动调整。

------

### 5. 评估指标：怎么算“好”？

句向量通常通过 **STS (Semantic Textual Similarity) 任务** 来评估。

- **数据集**：包含成千上万对句子，以及人工打分的相似度（0-5分）。
- **测试方法**：
  1. 用你的模型算出句向量，计算余弦相似度。
  2. 看**机器算的相似度**与**人工打分**的**相关系数 (Correlation)**。
  3. 相关系数越高（越接近 1），说明句向量越懂人类的逻辑。

## 第七部分：文本相似度计算 (Text Similarity)

### 1. 核心问题：怎么判断两句话意思像不像？

当我们把句子转换成向量（坐标点）后，问题就变成了：**如何衡量两个坐标点之间的关系？**

- **场景模拟**：
  - 用户搜：*"好"* (向量 A)
  - 文章 1：*"好 好 好 好 好"* (向量 B)
  - 文章 2：*"坏"* (向量 C)

### 2. 陷阱：欧氏距离 (Euclidean Distance)

这是初学者最容易踩的坑。

- **定义**：空间中两点之间的直线距离（用尺子量）。
- **公式**：$\|A - B\| = \sqrt{\sum (A_i - B_i)^2}$
- **为什么它在 NLP 里不好用？**
  - **长度敏感**：
    - 向量 A ("好") 的模长很短。
    - 向量 B ("好" x 5) 因为叠加了 5 次，模长非常长，在空间里离原点很远。
  - **误判**：
    - 欧氏距离会认为 A 和 B 离得很远（因为长度差太多）。
    - 反而可能觉得 A 和 C ("坏") 离得更近（因为都很短）。
  - **结论**：它过于关注“量”（文本长度），忽略了“质”（语义方向）。

### 3. 黄金标准：余弦相似度 (Cosine Similarity)

这是工业界（搜索、推荐、RAG）的默认选择。

> **下午纪要重点**：余弦相似度看的是**方向 (Direction)**，而不是距离。

- **几何直觉**：

  - 无论向量 B 伸得有多长，只要它和向量 A 指向**同一个方向**（夹角为 0），它们就是绝对相似的。
  - *"好"* 和 *"好 好 好"* 的夹角是 0 度 -> 相似度 1.0。
  - *"好"* 和 *"坏"* 的夹角接近 180 度 -> 相似度 -1.0。

- **数学公式**：

  $$\text{Similarity} = \cos(\theta) = \frac{A \cdot B}{\|A\| \times \|B\|}$$

  - **分子**：点积 (Dot Product)。衡量两个向量在方向上的累积贡献。
  - **分母**：模长乘积。起到**归一化 (Normalization)** 的作用，把长短的影响消除掉。

### 4. 代码逻辑深度剖析

> 💻 **代码对应**：请打开 `03_句向量与文本相似度.py` 第 50 行的 `calculate_similarity` 函数。

这段代码完全是数学公式的直接翻译：

Python

```
def calculate_similarity(vec1, vec2):
    # 1. 分子：点积
    # 如果两个向量方向一致，点积很大；垂直则为0
    numerator = np.dot(vec1, vec2)
    
    # 2. 分母：模长 (L2 Norm) 乘积
    # np.linalg.norm 就是求向量的长度
    denominator = np.linalg.norm(vec1) * np.linalg.norm(vec2)
    
    # 3. 安全保护
    if denominator == 0:
        return 0
        
    # 4. 归一化结果
    return numerator / denominator
```

### 5. 进阶优化：结合 TF-IDF

虽然代码 `03` 用的是基础版，但在实际项目中，我们通常会引入 TF-IDF 权重。

- **逻辑**：
  - 两个句子都有“的”，这不能说明它们相似。
  - 两个句子都有“**量子纠缠**”，这强有力地说明它们相似。
- **改进计算**：
  - 在计算点积时，给高 IDF（稀有）词的维度赋予更高的权重，强行拉近含有关键词的句子距离。

### 6. 实战落地：智能问答系统的流水线 (Pipeline)

结合代码 `03` 的 `find_answer` 函数，一个标准的问答系统是这样工作的：

1. **知识库入库 (Indexing)**：
   - 把 Excel 里 1000 个标准问题全算好句向量，存成一个矩阵 $M$。
2. **用户提问 (Query)**：
   - 用户问："怎么学编程？" -> 算出向量 $Q$。
3. **相似度检索 (Retrieval)**：
   - 拿 $Q$ 和矩阵 $M$ 做矩阵乘法（一次性算出 1000 个余弦相似度）。
4. **排序 (Ranking)**：
   - 找出得分最高的 Top 1。
   - *比如*："学习编程的方法" (0.89) > "编程是什么" (0.65)。
5. **阈值判断 (Thresholding)**：
   - 如果最高分只有 0.2，说明库里没有相关问题，系统回复：“我还需要学习...”。

## 第八部分：高级应用与展望 (Advanced Applications & Future)

### 1. 传统词向量的“阿喀琉斯之踵”

我们在前几节学的 Word2Vec、GloVe、FastText，统称为**静态词向量 (Static Word Embeddings)**。它们有一个无法克服的物理缺陷。

- **核心痛点：一词多义 (Polysemy)**
  - **场景**：
    - 句子 A："我喜欢吃**苹果**" (水果)。
    - 句子 B："**苹果**股价大跌" (公司)。
  - **静态向量的表现**：
    - 在 Word2Vec 模型里，`model.wv['苹果']` 永远对应同一个固定的向量（比如 `[0.1, 0.5, -0.3]`）。
    - 模型**无法区分**这两个“苹果”的区别，这导致在复杂的语义理解任务中（如阅读理解、情感分析），传统模型很容易“理解偏差”。

### 2. 破局者：BERT 与动态向量 (Dynamic Embeddings)

为了解决这个问题，Google 在 2018 年提出了 **BERT (Bidirectional Encoder Representations from Transformers)**。这是 NLP 历史上的**奇点**时刻。

#### **(1) 核心原理：语境决定意义**

BERT 不再给每个词发一个固定的“身份证”，而是根据上下文**实时生成**向量。

- **对比逻辑**：
  - **Word2Vec**：查字典模式。词库里存好了每个词的向量，用的时候直接查。
  - **BERT**：函数模式。输入一整句话，经过 12 层神经网络的复杂计算（Self-Attention），输出每个词在**当前语境下**的向量。
- **效果**：
  - 在句子 A 中，“苹果”的向量会非常靠近“香蕉”、“梨”。
  - 在句子 B 中，“苹果”的向量会非常靠近“微软”、“谷歌”。
  - **同一个词，向量是动态变化的。**

#### **(2) 句向量的终极形态：[CLS] 标记**

还记得我们在第六节讲的“平均池化”吗？BERT 给出了一个更优雅的方案。

- **机制**：
  - BERT 在处理每一句话时，都会在开头强制加一个特殊符号 `[CLS]`。
  - 经过深层网络的“信息搅拌”，`[CLS]` 位置的向量就自动浓缩了整句话的语义。
  - **实战价值**：工业界现在做文本分类，基本不再用 Word2Vec 平均了，直接取 BERT 的 `[CLS]` 向量接一个分类器，准确率通常能提升 10% 以上。

### 3. 从理解到生成：大模型 (LLMs) 的崛起

> **纪要补充**：下午课程提到了 DeepSeek、Gemini 等模型。

BERT 虽然强，但它主要强在**“理解”**（比如分类、找答案）。现在的 \**GPT\** 系列（包括 DeepSeek, Claude, Gemini）则更进一步，掌握了**“生成”**的能力。

- **演进路径**：
  - **Word2Vec (2013)**：让计算机认识了**词**。
  - **BERT (2018)**：让计算机读懂了**句子**（理解）。
  - **GPT/LLM (2020+)**：让计算机学会了**说话**（生成）。

### 4. 前沿探索：多模态 (Multi-modal) 与 CLIP

> **PPT 重点**：课程最后提到的图文结合。

未来的 NLP 不再只是处理文字，而是要打通文字与图像、视频的界限。

- **代表模型：CLIP (OpenAI)**
- **原理**：
  - 让模型同时看几亿张图和对应的文字描述。
  - 强行把“一张狗的照片”的向量，和“一只狗”的文本向量拉近。
- **应用**：
  - **以文搜图**：你在搜索框输入“落日下的海滩”，系统能直接搜出对应的照片，而不需要照片上有任何标签。

------

### **全课知识体系总结 (复习导图)**

为了方便您最终复习，我把这八节课的内容浓缩成一条逻辑线：

1. **起点**：One-Hot 编码太烂（维度灾难、没语义）。
2. **基石**：**Word2Vec** 横空出世，用**低维稠密向量**解决了语义问题。
   - *算法*：Skip-gram (小数据首选) vs CBOW。
   - *加速*：负采样。
3. **修补**：
   - **GloVe**：用全局统计修补了局部视野。
   - **FastText**：用子词 (N-gram) 修补了不认识生词 (OOV) 的问题。
4. **实战**：
   - **词 -> 句**：平均池化 (简单) / TF-IDF 加权 (进阶)。
   - **相似度**：**余弦相似度** (看方向) 优于 欧氏距离 (看长短)。
5. **未来**：**BERT** 用动态向量解决了多义词问题，开启了大模型时代。