# 机器学习与数据挖掘全流程笔记 (第二课·进阶版)

## 一、 数据构造与生成 (Data Generation)

在没有真实数据时，我们需要自己“造”数据来测试代码。这主要用到 `NumPy` 的随机模块和 `Pandas`。

### 1. 随机数生成 (NumPy Random)

- `**np.random.seed(seed)**`
    - **功能**：设置随机种子。
    - **通俗理解**：就像给随机播放器定个“歌单”。只要种子一样，每次生成的随机数序列都一模一样，方便复现结果。

- `**np.random.randint(low, high, size)**`
    - **功能**：生成随机整数。
    - **案例**：`np.random.randint(18, 65, 200)` -> 生成200个18到64岁的年龄数据。

- `**np.random.normal(loc, scale, size)**`
    - **功能**：生成符合**正态分布**（高斯分布）的数据。
    - **参数**：
        - `loc` (均值)：比如平均工资 8000。
        - `scale` (标准差)：比如工资波动范围 2000。
    - **场景**：模拟身高、体重、工资这种大多数人集中在中间，两头少的数据。

- `**np.random.uniform(low, high, size)**`
    - **功能**：生成**均匀分布**的小数。
    - **场景**：模拟评分（1.0 到 10.0），每个分数出现的概率差不多。

- `**np.random.choice(a, size, replace=True)**`
    - **功能**：从一个列表里随机抽取。
    - **参数**：
        - `replace=True`：抽完放回（可以重复抽到同一个）。
        - `replace=False`：抽完不放回（不能重复）。
    - **案例**：`np.random.choice(['技术部', '销售部'], 200)` -> 给员工随机分配部门。

### 2. DataFrame 操作与噪音注入

- `**pd.concat([df1, df2], ignore_index=True)**`
    - **功能**：把两个表格上下拼接起来。
    - **场景**：把原始数据和我们特意生成的“重复数据”拼在一起，制造脏数据。

- `**df.sample(frac=1, random_state=42)**`
    - **功能**：随机打乱表格的行顺序。
    - **参数**：`frac=1` 表示抽取 100% 的数据（也就是全部打乱）。

- `**np.nan**`
    - **功能**：表示缺失值 (Not a Number)。
    - **技巧**：`df.loc[indices, '列名'] = np.nan` -> 强行把某些位置挖空，模拟数据丢失。

---

## 二、 数据清洗与预处理 (Data Cleaning)

拿到数据（尤其是脏数据）后的标准处理流水线。

### 1. 数据概览 (Inspection)

- `**df.select_dtypes(include=[np.number])**`
    - **功能**：只把数字类型的列挑出来。
    - **场景**：计算平均值或检测异常值时，我们只关心数字列，不关心“姓名”这种文字列。

### 2. 重复值处理 (Duplicates)

- `**df.duplicated(keep=False)**`
    - **功能**：标记重复的行。
    - **参数**：`keep=False` 表示把所有重复的行都标出来（包括第一次出现的），方便查看。

- `**df.drop_duplicates()**`
    - **功能**：删除重复行，只保留一份。

### 3. 缺失值处理 (Missing Values)

- `**df.isnull().sum()**`
    - **功能**：统计每列有多少个空值。

- `**df.fillna(value, inplace=True)**`
    - **功能**：填补空缺。
    - **常用策略**：
        - **中位数填充** (`df[col].median()`)：适合数值型，不受极大/极小值影响。
        - **众数填充** (`df[col].mode()[0]`)：适合类别型（如填充出现最多的部门）。

- `**df.dropna()**`
    - **功能**：如果有没法填的空值，直接删掉那一行。

### 4. 异常值检测与处理 (Outliers)

#### 方法 A：IQR (四分位距法)
*适合不那么符合正态分布的数据，比较稳健。*

- `**df[col].quantile(0.25)**` / `**0.75**`
    - **功能**：计算 25% 分位点 (Q1) 和 75% 分位点 (Q3)。
- **计算公式**：
    - `IQR = Q3 - Q1` (箱子的宽度)
    - `下界 = Q1 - 1.5 * IQR`
    - `上界 = Q3 + 1.5 * IQR`
- **判定**：小于下界或大于上界的都是异常值。

#### 方法 B：Z-score (标准分数法)
*适合符合正态分布的数据。*

- **计算公式**：`(数值 - 均值) / 标准差`
- **判定**：结果的绝对值大于 3（即偏离均值超过 3 个标准差），通常视为异常。

#### 方法 C：Winsorization (盖帽法/缩尾)
- **操作**：不删除异常值，而是把它们“压”回边界上。
- **代码实现**：
```python
# 比上界还大的，强行改成上界值
df.loc[df[col] > upper_bound, col] = upper_bound
```

## 三、 监督学习模型 (Regression & Classification)

### 1. 线性回归 (Linear Regression)

*预测“多少” (连续数值)*

- `**LinearRegression()**`
  - `.fit(X, y)`：训练模型。
  - `.predict(X)`：预测结果。
  - `.coef_`：**回归系数**（斜率）。表示特征每增加 1 个单位，结果变化多少。
  - `.intercept_`：**截距**。当特征为 0 时的基础值。

### 2. 多项式回归 (Polynomial Regression)

*处理弯曲的数据关系*

- `**PolynomialFeatures(degree=n)**`
  - **功能**：特征升维。
  - **参数**：`degree` (次数)。次数越高，曲线越复杂，越能拟合扭曲的数据，但也越容易过拟合。
  - **流程**：先用这个工具把 `X` 变成 `X_poly`（包含 $X^2, X^3$...），再扔给线性回归模型去训练。

### 3. 逻辑回归 (Logistic Regression)

预测“是哪个” (分类)

虽然叫“回归”，但其实是用回归的数学方法做分类。

- `**LogisticRegression(multi_class='multinomial')**`
  - **功能**：分类器。
  - **参数**：
    - `multi_class='multinomial'`：明确告诉模型我们要处理多分类问题（如鸢尾花有3类）。
  - **关键方法**：
    - `.predict(X)`：直接告诉你类别（如 0 或 1）。
    - `.predict_proba(X)`：告诉你**概率**（如 80% 是类别 1，20% 是类别 0）。**这是画 ROC 曲线的基础。**

------

## 四、 模型评估指标 (Model Evaluation)

模型好不好，不能只看大概，要看具体的“体检报告”。

### 1. 回归模型指标 (数值预测)

- `**mean_squared_error (MSE)**` **均方误差**
  - **含义**：误差平方的平均值。对**大误差**非常敏感（惩罚重）。
- `**mean_absolute_error (MAE)**` **平均绝对误差**
  - **含义**：误差绝对值的平均值。对异常值不那么敏感，更客观反映平均偏离程度。
- `**r2_score (R²)**` **决定系数**
  - **含义**：模型解释了多少数据的波动。越接近 1 越好。如果是 0，说明模型和“瞎猜平均值”一样烂。

### 2. 分类模型指标 (类别判断)

#### 基础指标

- `**accuracy_score**` **(准确率)**：答对的题 / 总题数。
- `**precision_score**` **(精确率)**：预测为“真”的里面，有多少是真的？（关注**误报**）。
- `**recall_score**` **(召回率)**：真实的“真”里面，被你找出来多少？（关注**漏报**）。
- `**f1_score**`：精确率和召回率的平衡分数。

#### 进阶工具

- `**confusion_matrix(y_true, y_pred)**` **(混淆矩阵)**
  - **功能**：生成一个矩阵，展示 TP, TN, FP, FN。配合 `sns.heatmap` 画热力图非常直观。
- `**classification_report**`
  - **功能**：一键生成包含准确率、精确率、召回率、F1 的详细报表。

#### 曲线与阈值 (Thresholds)

*模型输出的是概率，通过调整阈值（默认 0.5），可以在精确率和召回率之间做权衡。*

- `**roc_curve(y_true, y_score)**`
  - **功能**：计算不同阈值下的 FPR (假阳性率) 和 TPR (召回率)。
  - **用途**：绘制 ROC 曲线。
- `**auc(fpr, tpr)**`
  - **功能**：计算 ROC 曲线下的面积。AUC 越大（越接近 1），模型区分正负样本的能力越强。
- `**precision_recall_curve(y_true, y_score)**`
  - **功能**：计算不同阈值下的精确率和召回率。
  - **用途**：绘制 PR 曲线，适合**类别极度不平衡**的情况。

------

## 五、 可视化绘图技巧 (Matplotlib & Seaborn)

代码中出现的高级绘图方法。

- **中文字体设置 (Tips)**
  - `fm.FontProperties(fname=path)`：在代码中专门写了一个 `setup_chinese_font` 函数，通过加载本地字体文件解决 Matplotlib 显示中文乱码的问题。
- **分类边界可视化**
  - `np.meshgrid`：生成网格点。
  - `plt.contourf`：在网格上画等高线（填充颜色），用来展示分类模型的**决策边界**（不同颜色代表不同区域）。
- **多子图布局**
  - `plt.subplots(nrows, ncols)`：一次性创建多个图表。
  - `ax.text()`：在图上写字（代码中用来写“指标解释”的小卡片）。
- **Seaborn 高级图表**
  - `sns.heatmap`：画热力图（用于混淆矩阵）。
  - `sns.set_style("whitegrid")`：设置背景网格样式，让图表更专业。